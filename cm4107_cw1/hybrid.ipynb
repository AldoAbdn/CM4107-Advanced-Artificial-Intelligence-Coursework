{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM4107 Advanced Artificial Intelligence\n",
    "## Coursework Part 1 - ANN and kNN Hybrid\n",
    "## Alistair Quinn 1701183"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.special \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train and Test Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'wine':{},'breast_cancer':{}}\n",
    "#Dataset 1: Wine\n",
    "file = open('datasets/wine/wine_train.csv','r')\n",
    "file.readline()\n",
    "datasets['wine']['train'] = file.readlines()\n",
    "file.close()\n",
    "file = open('datasets/wine/wine_test.csv','r')\n",
    "file.readline()\n",
    "datasets['wine']['test'] = file.readlines()\n",
    "file.close()\n",
    "#Dataset 2: Breast Cancer\n",
    "file = open('datasets/breast_cancer/breast_cancer_train.csv','r')\n",
    "file.readline()\n",
    "datasets['breast_cancer']['train'] = file.readlines()\n",
    "file.close()\n",
    "file = open('datasets/breast_cancer/breast_cancer_train.csv', 'r')\n",
    "file.readline()\n",
    "datasets['breast_cancer']['test'] = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 1: Wine\n",
    "wine_train = pd.read_csv('datasets/wine/wine_train.csv')\n",
    "datasets['wine']['train_x'] = wine_train.iloc[:,1:12]\n",
    "datasets['wine']['train_y'] = wine_train.iloc[:,0:1]\n",
    "wine_test = pd.read_csv('datasets/wine/wine_test.csv')\n",
    "datasets['wine']['test_x'] = wine_test.iloc[:,1:12]\n",
    "datasets['wine']['test_y'] = wine_test.iloc[:,0:1]\n",
    "#Dataset 2: Breast Cancer \n",
    "breast_cancer_train = pd.read_csv('datasets/breast_cancer/breast_cancer_train.csv')\n",
    "datasets['breast_cancer']['train_x'] = breast_cancer_train.iloc[:,1:33]\n",
    "datasets['breast_cancer']['train_y'] = breast_cancer_train.iloc[:,0:1]\n",
    "breast_cancer_test = pd.read_csv('datasets/breast_cancer/breast_cancer_test.csv')\n",
    "datasets['breast_cancer']['test_x'] = breast_cancer_test.iloc[:,1:33]\n",
    "datasets['breast_cancer']['test_y'] = breast_cancer_test.iloc[:,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can test to see how many of the test instances we got correct\n",
    "def accuracy(results):\n",
    "    correct = 0\n",
    "    for predict, target in results:\n",
    "        if predict == target:\n",
    "            correct += 1\n",
    "    return (correct/float(len(results))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the idea values from testing ANN and kNN (ann.ipynb and knn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wine Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN\n",
    "datasets['wine']['inputnodes'] = 11\n",
    "datasets['wine']['hiddennodes'] = 10\n",
    "datasets['wine']['outputnodes'] = 11\n",
    "datasets['wine']['epochs'] = 250\n",
    "datasets['wine']['lr'] = 0.1 #Learning Rate\n",
    "datasets['wine']['bs'] = 1 #Batch Size\n",
    "#KNN\n",
    "datasets['wine']['k'] = 20\n",
    "datasets['wine']['weighted'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breast Cancer Datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN\n",
    "datasets['breast_cancer']['inputnodes'] = 30\n",
    "datasets['breast_cancer']['hiddennodes'] = 15\n",
    "datasets['breast_cancer']['outputnodes'] = 2\n",
    "datasets['breast_cancer']['epochs'] = 260\n",
    "datasets['breast_cancer']['lr'] = 1.0 #Learning Rate\n",
    "datasets['breast_cancer']['bs'] = 1 #Batch Size\n",
    "#KNN\n",
    "datasets['breast_cancer']['k'] = 10\n",
    "datasets['breast_cancer']['weighted'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    \"\"\"Artificial Neural Network classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    lr : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    ep : int\n",
    "      Number of epochs for training the network towards achieving convergence\n",
    "    batch_size : int\n",
    "      Size of the training batch to be used when calculating the gradient descent. \n",
    "      batch_size = 0 standard gradient descent\n",
    "      batch_size > 0 stochastic gradient descent \n",
    "\n",
    "    inodes : int\n",
    "      Number of input nodes which is normally the number of features in an instance.\n",
    "    hnodes : int\n",
    "      Number of hidden nodes in the net.\n",
    "    onodes : int\n",
    "      Number of output nodes in the net.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    wih : 2d-array\n",
    "      Input2Hidden node weights after fitting \n",
    "    who : 2d-array\n",
    "      Hidden2Output node weights after fitting \n",
    "    E : list\n",
    "      Sum-of-squares error value in each epoch.\n",
    "      \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.\n",
    "      \n",
    "    Functions\n",
    "    ---------\n",
    "    activation_function : float (between 1 and -1)\n",
    "        implments the sigmoid function which squashes the node input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputnodes=784, hiddennodes=200, outputnodes=10, learningrate=0.1, batch_size=1, epochs=10):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        #link weight matrices, wih (input to hidden) and who (hidden to output)\n",
    "        #a weight on link from node i to node j is w_ij\n",
    "        \n",
    "        \n",
    "        #Draw random samples from a normal (Gaussian) distribution centered around 0.\n",
    "        #numpy.random.normal(loc to centre gaussian=0.0, scale=1, size=dimensions of the array we want) \n",
    "        #scale is usually set to the standard deviation which is related to the number of incoming links i.e. \n",
    "        #1/sqrt(num of incoming inputs). we use pow to raise it to the power of -0.5.\n",
    "        #We have set 0 as the centre of the guassian dist.\n",
    "        # size is set to the dimensions of the number of hnodes, inodes and onodes\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #set the learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        #set the batch size\n",
    "        self.bs = batch_size\n",
    "        \n",
    "        #set the number of epochs\n",
    "        self.ep = epochs\n",
    "        \n",
    "        #store errors at each epoch\n",
    "        self.E= []\n",
    "        \n",
    "        #store results from testing the model\n",
    "        #keep track of the network performance on each test instance\n",
    "        self.results= []\n",
    "        \n",
    "        #define the activation function here\n",
    "        #specify the sigmoid squashing function. Here expit() provides the sigmoid function.\n",
    "        #lambda is a short cut function which is executed there and then with no def (i.e. like an anonymous function)\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "   \n",
    "    def batch_input(self, input_list):\n",
    "        \"\"\"Yield consecutive batches of the specified size from the input list.\"\"\"\n",
    "        for i in range(0, len(input_list), self.bs):\n",
    "            yield input_list[i:i + self.bs]\n",
    "    \n",
    "    #train the neural net\n",
    "    #note the first part is very similar to the query function because they both require the forward pass\n",
    "    def train(self, train_inputs):\n",
    "        \"\"\"Training the neural net. \n",
    "           This includes the forward pass ; error computation; \n",
    "           backprop of the error ; calculation of gradients and updating the weights.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_inputs : {array-like}, shape = [n_instances, n_features]\n",
    "            Training vectors, where n_instances is the number of training instances and\n",
    "            n_features is the number of features.\n",
    "            Note this contains all features including the class feature which is in first position\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            self : object\n",
    "        \"\"\"\n",
    "      \n",
    "        for e in range(self.ep):\n",
    "            print(\"Training epoch#: \", e)\n",
    "            sum_error = 0.0   \n",
    "            for batch in self.batch_input(train_inputs):\n",
    "                #creating variables to store the gradients   \n",
    "                delta_who = 0\n",
    "                delta_wih = 0\n",
    "                \n",
    "                # iterate through the inputs sent in\n",
    "                for instance in batch:\n",
    "                    # split it by the commas\n",
    "                    all_values = instance.split(',') \n",
    "                    # scale and shift the inputs to address the problem of diminishing weights due to multiplying by zero\n",
    "                    # divide the raw inputs which are in the range 0-255 by 255 will bring them into the range 0-1\n",
    "                    # multiply by 0.99 to bring them into the range 0.0 - 0.99.\n",
    "                    # add 0.01 to shift them up to the desired range 0.01 - 1. \n",
    "                    inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "                    #create the target output values for each instance so that we can use it with the neural net\n",
    "                    #note we need 10 nodes where each represents one of the digits\n",
    "                    targets = np.zeros(self.onodes) + 0.01 #all initialised to 0.01\n",
    "                    #all_value[0] has the target class label for this instance\n",
    "                    targets[int(all_values[0])] = 0.99\n",
    "        \n",
    "                    #convert  inputs list to 2d array\n",
    "                    inputs = np.array(inputs,  ndmin=2).T\n",
    "                    targets = np.array(targets, ndmin=2).T\n",
    "\n",
    "                    #calculate signals into hidden layer\n",
    "                    hidden_inputs = np.dot(self.wih, inputs)\n",
    "                    #calculate the signals emerging from the hidden layer\n",
    "                    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "                    #calculate signals into final output layer\n",
    "                    final_inputs=np.dot(self.who, hidden_outputs)\n",
    "                    #calculate the signals emerging from final output layer\n",
    "                    final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "                    #to calculate the error we need to compute the element wise diff between target and actual\n",
    "                    output_errors = targets - final_outputs\n",
    "                    #Next distribute the error to the hidden layer such that hidden layer error\n",
    "                    #is the output_errors, split by weights, recombined at hidden nodes\n",
    "                    hidden_errors = np.dot(self.who.T, output_errors)\n",
    "            \n",
    "                       \n",
    "                    ## for each instance accumilate the gradients from each instance\n",
    "                    ## delta_who are the gradients between hidden and output weights\n",
    "                    ## delta_wih are the gradients between input and hidden weights\n",
    "                    delta_who += np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "                    delta_wih += np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "                    \n",
    "                    sum_error += np.dot(output_errors.T, output_errors)#this is the sum of squared error accumilated over each batced instance\n",
    "                   \n",
    "                pass #instance\n",
    "            \n",
    "                # update the weights by multiplying the gradient with the learning rate\n",
    "                # note that the deltas are divided by batch size to obtain the average gradient according to the given batch\n",
    "                # obviously if batch size = 1 then we dont need to bother with an average\n",
    "                self.who += self.lr * (delta_who / self.bs)\n",
    "                self.wih += self.lr * (delta_wih / self.bs)\n",
    "            pass # batch\n",
    "            self.E.append(np.asfarray(sum_error).flatten())\n",
    "            print(\"errors (SSE): \", self.E[-1])\n",
    "        pass # epoch\n",
    "    \n",
    "    #query the neural net\n",
    "    def query(self, inputs_list):\n",
    "        #convert inputs_list to a 2d array\n",
    "        #print(numpy.matrix(inputs_list))\n",
    "        #inputs_list [[ 1.   0.5 -1.5]]\n",
    "        inputs = np.array(inputs_list, ndmin=2).T \n",
    "        #once converted it appears as follows\n",
    "        #[[ 1. ]\n",
    "        # [ 0.5]\n",
    "        # [-1.5]]\n",
    "        #print(numpy.matrix(inputs))\n",
    "        \n",
    "        #propogate input into hidden layer. This is the start of the forward pass\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        \n",
    "        #squash the content in the hidden node using the sigmoid function (value between 1, -1)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "                \n",
    "        #propagate into output layer and the apply the squashing sigmoid function\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "    \n",
    "     \n",
    "    #iterate through all the test data to calculate model accuracy\n",
    "    def test(self, test_inputs):\n",
    "        self.results = []\n",
    "        \n",
    "        #go through each test instances\n",
    "        for instance in test_inputs:\n",
    "            all_values = instance.split(',') # extract the input feature values for the instance\n",
    "            target_label = int(all_values[0]) # get the target class for the instance\n",
    "    \n",
    "            #scale and shift the inputs this is to make sure values dont lead to zero when multiplied with weights\n",
    "            inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "            #query the network with test inputs\n",
    "            #note this returns 10 output values ; of which the index of the highest value\n",
    "            # is the networks predicted class label\n",
    "            outputs = self.query(inputs)\n",
    "    \n",
    "            #get the index of the highest output node as this corresponds to the predicted class\n",
    "            predict_label = np.argmax(outputs) #this is the class predicted by the ANN\n",
    "    \n",
    "            self.results.append([predict_label, target_label])\n",
    "            #compute network error\n",
    "            ##if (predict_label == target_label):\n",
    "              ##  self.results.append(1)\n",
    "            ##else: \n",
    "             ##   self.results.append(0)\n",
    "            pass\n",
    "        pass\n",
    "        self.results = np.asfarray(self.results) # flatten results to avoid nested arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan(instance1, instance2):\n",
    "        '''\n",
    "        Calculates manhattan distance between two instances of data\n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        distance = 0\n",
    "        for val1, val2 in zip(instance1, instance2):\n",
    "            distance += abs(val1 - val2)      \n",
    "              \n",
    "        return 1 / (1+ distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    X_train, Y_train : list\n",
    "    these consists of the training set feature values and associated class labels\n",
    "    k : int\n",
    "    specify the number of neighbours\n",
    "    sim : literal\n",
    "    specify the name of the similarity metric (e.g. manhattan, eucliedean)\n",
    "    weighted : Boolean\n",
    "    specify the voting strategy as weighted or not weighted by similarity values\n",
    "  \n",
    "    Attributes\n",
    "    -----------  \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, X_train, Y_train, k=3, sim=manhattan, weighted=False):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        if k <= len(self.X_train):\n",
    "            self.k = k # set the k value for neighbourhood size\n",
    "        else:\n",
    "            self.k = len(self.X_train) # to ensure the get_neighbours dont crash\n",
    "    \n",
    "        self.similarity = sim # specify a sim metric that has been pre-defined e.g. manhattan or euclidean\n",
    "        \n",
    "        self.weighted = weighted # boolean to choose between weighted / unweighted majority voting\n",
    "        \n",
    "        #store results from testing \n",
    "        self.results= []\n",
    "        \n",
    "    #With k-NN, we are interested in finding the k number of points with the greatest similarity \n",
    "    # to the the query or test instance.\n",
    "    def get_neighbours(self, test_instance):\n",
    "        '''\n",
    "        Locate most similar neighbours \n",
    "        X_train will be a containing features (Float) values (i.e. your training data)\n",
    "        Y_train will be the corresponding class labels for each instance in X_train\n",
    "        test_instance will be a List of Float values (i.e. a query instance)\n",
    "        '''\n",
    "        similarities = [] # collection to store the similarities to be computed\n",
    "\n",
    "        for train_instance, y in zip(self.X_train, self.Y_train): #for each member of the training set\n",
    "            sim = self.similarity(test_instance, train_instance) #calculate the similarity to the test instance\n",
    "            \n",
    "            similarities.append((y, sim)) #add the actual label of the example and the computed similarity to a collection \n",
    "        #print(distances)\n",
    "        similarities.sort(key = operator.itemgetter(1), reverse = True) #sort the collection by decreasing similarity\n",
    "        neighbours = [] # holds the k most similar neighbours\n",
    "        for x in range(self.k): #extract the k top indices of the collection for return\n",
    "            neighbours.append(similarities[x])\n",
    "\n",
    "        return neighbours\n",
    "\n",
    "    # given the neighbours make a prediction\n",
    "    # the boolean parameter when set to False will use unweighted majority voting; otherwise weighted majority voting\n",
    "    # weighting can be helpful to break any ties in voting\n",
    "    def predict(self, neighbours):\n",
    "        '''\n",
    "        Summarise a prediction based upon weighted neighbours calculation\n",
    "        '''\n",
    "        class_votes = {}\n",
    "        for x in range(len(neighbours)):\n",
    "            response = neighbours[x][0]\n",
    "            #Quick Check So I don't have to mess around changing loaded datasets\n",
    "            if(not isinstance(response,int)):\n",
    "                response = response[0]\n",
    "            if response in class_votes:\n",
    "                class_votes[response] += (1-self.weighted) + (self.weighted * neighbours[x][1]) #if not weighted simply add 1\n",
    "                #class_votes[response] += [1, neighbours[x][1]][weighted == True] \n",
    "              \n",
    "            else:\n",
    "                class_votes[response] = (1-self.weighted) + (self.weighted * neighbours[x][1])\n",
    "                #class_votes[response] = [1, neighbours[x][1]][weighted == True] \n",
    "                \n",
    "        #print(class_votes)\n",
    "        sorted_votes = sorted(class_votes, key = lambda k: (class_votes[k], k), reverse = True)\n",
    "        #print(sorted_votes)\n",
    "        return sorted_votes[0]\n",
    "    \n",
    "    #iterate through all the test data to calculate accuracy\n",
    "    def test(self, X_test, Y_test):\n",
    "        self.results = [] # store the predictions returned by kNN\n",
    "\n",
    "        for test_instance, target_label in zip(X_test, Y_test):\n",
    "            neighbours = self.get_neighbours(test_instance)\n",
    "            predict_label = self.predict(neighbours)\n",
    "            self.results.append([predict_label, target_label])\n",
    "            #print('> predicted = ', result,', actual = ', test_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN-kNN Hybrid Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNkNNHybrid:\n",
    "    def __init__(self, inputnodes=784, hiddennodes=200, outputnodes=10, learningrate=0.1, batch_size=1, epochs=10, k=3, sim=manhattan,weighted=False):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        self.k = k\n",
    "        self.sim = sim\n",
    "        self.weighted = weighted\n",
    "        #link weight matrices, wih (input to hidden) and who (hidden to output)\n",
    "        #a weight on link from node i to node j is w_ij\n",
    "        \n",
    "        \n",
    "        #Draw random samples from a normal (Gaussian) distribution centered around 0.\n",
    "        #numpy.random.normal(loc to centre gaussian=0.0, scale=1, size=dimensions of the array we want) \n",
    "        #scale is usually set to the standard deviation which is related to the number of incoming links i.e. \n",
    "        #1/sqrt(num of incoming inputs). we use pow to raise it to the power of -0.5.\n",
    "        #We have set 0 as the centre of the guassian dist.\n",
    "        # size is set to the dimensions of the number of hnodes, inodes and onodes\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #set the learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        #set the batch size\n",
    "        self.bs = batch_size\n",
    "        \n",
    "        #set the number of epochs\n",
    "        self.ep = epochs\n",
    "        \n",
    "        #store errors at each epoch\n",
    "        self.E= []\n",
    "        \n",
    "        #store results from testing the model\n",
    "        #keep track of the network performance on each test instance\n",
    "        self.results= []\n",
    "        \n",
    "        #define the activation function here\n",
    "        #specify the sigmoid squashing function. Here expit() provides the sigmoid function.\n",
    "        #lambda is a short cut function which is executed there and then with no def (i.e. like an anonymous function)\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "   \n",
    "    def batch_input(self, input_list):\n",
    "        \"\"\"Yield consecutive batches of the specified size from the input list.\"\"\"\n",
    "        for i in range(0, len(input_list), self.bs):\n",
    "            yield input_list[i:i + self.bs]\n",
    "    \n",
    "    #train the neural net\n",
    "    #note the first part is very similar to the query function because they both require the forward pass\n",
    "    def train(self, train_inputs):\n",
    "        \"\"\"Training the neural net. \n",
    "           This includes the forward pass ; error computation; \n",
    "           backprop of the error ; calculation of gradients and updating the weights.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_inputs : {array-like}, shape = [n_instances, n_features]\n",
    "            Training vectors, where n_instances is the number of training instances and\n",
    "            n_features is the number of features.\n",
    "            Note this contains all features including the class feature which is in first position\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            self : object\n",
    "        \"\"\"\n",
    "      \n",
    "        for e in range(self.ep):\n",
    "            print(\"Training epoch#: \", e)\n",
    "            sum_error = 0.0   \n",
    "            for batch in self.batch_input(train_inputs):\n",
    "                #creating variables to store the gradients   \n",
    "                delta_who = 0\n",
    "                delta_wih = 0\n",
    "                \n",
    "                # iterate through the inputs sent in\n",
    "                for instance in batch:\n",
    "                    # split it by the commas\n",
    "                    all_values = instance.split(',') \n",
    "                    # scale and shift the inputs to address the problem of diminishing weights due to multiplying by zero\n",
    "                    # divide the raw inputs which are in the range 0-255 by 255 will bring them into the range 0-1\n",
    "                    # multiply by 0.99 to bring them into the range 0.0 - 0.99.\n",
    "                    # add 0.01 to shift them up to the desired range 0.01 - 1. \n",
    "                    inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "                    #create the target output values for each instance so that we can use it with the neural net\n",
    "                    #note we need 10 nodes where each represents one of the digits\n",
    "                    targets = np.zeros(self.onodes) + 0.01 #all initialised to 0.01\n",
    "                    #all_value[0] has the target class label for this instance\n",
    "                    targets[int(all_values[0])] = 0.99\n",
    "        \n",
    "                    #convert  inputs list to 2d array\n",
    "                    inputs = np.array(inputs,  ndmin=2).T\n",
    "                    targets = np.array(targets, ndmin=2).T\n",
    "\n",
    "                    #calculate signals into hidden layer\n",
    "                    hidden_inputs = np.dot(self.wih, inputs)\n",
    "                    #calculate the signals emerging from the hidden layer\n",
    "                    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "                    #calculate signals into final output layer\n",
    "                    final_inputs=np.dot(self.who, hidden_outputs)\n",
    "                    #calculate the signals emerging from final output layer\n",
    "                    final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "                    #to calculate the error we need to compute the element wise diff between target and actual\n",
    "                    output_errors = targets - final_outputs\n",
    "                    #Next distribute the error to the hidden layer such that hidden layer error\n",
    "                    #is the output_errors, split by weights, recombined at hidden nodes\n",
    "                    hidden_errors = np.dot(self.who.T, output_errors)\n",
    "            \n",
    "                       \n",
    "                    ## for each instance accumilate the gradients from each instance\n",
    "                    ## delta_who are the gradients between hidden and output weights\n",
    "                    ## delta_wih are the gradients between input and hidden weights\n",
    "                    delta_who += np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "                    delta_wih += np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "                    \n",
    "                    sum_error += np.dot(output_errors.T, output_errors)#this is the sum of squared error accumilated over each batced instance\n",
    "                   \n",
    "                pass #instance\n",
    "            \n",
    "                # update the weights by multiplying the gradient with the learning rate\n",
    "                # note that the deltas are divided by batch size to obtain the average gradient according to the given batch\n",
    "                # obviously if batch size = 1 then we dont need to bother with an average\n",
    "                self.who += self.lr * (delta_who / self.bs)\n",
    "                self.wih += self.lr * (delta_wih / self.bs)\n",
    "            pass # batch\n",
    "            self.E.append(np.asfarray(sum_error).flatten())\n",
    "            print(\"errors (SSE): \", self.E[-1])\n",
    "        #Query ANN with Test Inputs Again \n",
    "        #go through each test instances\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        for instance in train_inputs:\n",
    "            all_values = instance.split(',') # extract the input feature values for the instance\n",
    "            target_label = int(all_values[0]) # get the target class for the instance\n",
    "    \n",
    "            #scale and shift the inputs this is to make sure values dont lead to zero when multiplied with weights\n",
    "            inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "            #query the network with test inputs\n",
    "            #note this returns 10 output values ; of which the index of the highest value\n",
    "            # is the networks predicted class label\n",
    "            outputs = self.query_hidden(inputs)\n",
    "            \n",
    "            #Add converted instance to array\n",
    "            x_values.append(outputs)\n",
    "            \n",
    "            #Add Corresponding target to other array\n",
    "            y_values.append(target_label)\n",
    "        #Create kNN\n",
    "        self.kNN = kNN(x_values,y_values,self.k,self.sim,self.weighted)\n",
    "    \n",
    "    \n",
    "    #query the neural net\n",
    "    def query(self, inputs_list):\n",
    "        #convert inputs_list to a 2d array\n",
    "        #print(numpy.matrix(inputs_list))\n",
    "        #inputs_list [[ 1.   0.5 -1.5]]\n",
    "        inputs = np.array(inputs_list, ndmin=2).T \n",
    "        #once converted it appears as follows\n",
    "        #[[ 1. ]\n",
    "        # [ 0.5]\n",
    "        # [-1.5]]\n",
    "        #print(numpy.matrix(inputs))\n",
    "        \n",
    "        #propogate input into hidden layer. This is the start of the forward pass\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        \n",
    "        #squash the content in the hidden node using the sigmoid function (value between 1, -1)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "                \n",
    "        #propagate into output layer and the apply the squashing sigmoid function\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "    \n",
    "    def query_hidden(self,inputs_list):\n",
    "        #convert inputs_list to a 2d array\n",
    "        #print(numpy.matrix(inputs_list))\n",
    "        #inputs_list [[ 1.   0.5 -1.5]]\n",
    "        inputs = np.array(inputs_list, ndmin=2).T \n",
    "        #once converted it appears as follows\n",
    "        #[[ 1. ]\n",
    "        # [ 0.5]\n",
    "        # [-1.5]]\n",
    "        #print(numpy.matrix(inputs))\n",
    "        \n",
    "        #propogate input into hidden layer. This is the start of the forward pass\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        \n",
    "        #squash the content in the hidden node using the sigmoid function (value between 1, -1)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        return hidden_outputs\n",
    "     \n",
    "    #iterate through all the test data to calculate model accuracy\n",
    "    def test(self, test_inputs):\n",
    "        self.results = []\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        #go through each test instances\n",
    "        for instance in test_inputs:\n",
    "            all_values = instance.split(',') # extract the input feature values for the instance\n",
    "            target_label = int(all_values[0]) # get the target class for the instance\n",
    "    \n",
    "            #scale and shift the inputs this is to make sure values dont lead to zero when multiplied with weights\n",
    "            inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "            #query the network with test inputs\n",
    "            #note this returns 10 output values ; of which the index of the highest value\n",
    "            # is the networks predicted class label\n",
    "            outputs = self.query_hidden(inputs)\n",
    "    \n",
    "            #get the index of the highest output node as this corresponds to the predicted class\n",
    "            predict_label = np.argmax(outputs) #this is the class predicted by the ANN\n",
    "    \n",
    "            #Add outputs to x values\n",
    "            x_values.append(outputs)\n",
    "            \n",
    "            #Add target to target list\n",
    "            y_values.append(target_label)\n",
    "        #Test knn\n",
    "        self.kNN.test(x_values,y_values)\n",
    "        #Set results to kNN results\n",
    "        self.results = self.kNN.results\n",
    "        print(self.kNN.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Results of all 3 Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_results = []\n",
    "breast_cancer_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch#:  0\n",
      "errors (SSE):  [1743.69107385]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [1611.76213807]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [1609.05176636]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [1608.14560802]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [1607.71382762]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [1607.46931658]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [1607.31491648]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [1607.20923596]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [1607.13196724]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [1607.07213619]\n",
      "Training epoch#:  10\n",
      "errors (SSE):  [1607.02337202]\n",
      "Training epoch#:  11\n",
      "errors (SSE):  [1606.98176128]\n",
      "Training epoch#:  12\n",
      "errors (SSE):  [1606.94478456]\n",
      "Training epoch#:  13\n",
      "errors (SSE):  [1606.91075221]\n",
      "Training epoch#:  14\n",
      "errors (SSE):  [1606.87848803]\n",
      "Training epoch#:  15\n",
      "errors (SSE):  [1606.8471435]\n",
      "Training epoch#:  16\n",
      "errors (SSE):  [1606.81608453]\n",
      "Training epoch#:  17\n",
      "errors (SSE):  [1606.78481998]\n",
      "Training epoch#:  18\n",
      "errors (SSE):  [1606.7529552]\n",
      "Training epoch#:  19\n",
      "errors (SSE):  [1606.72016112]\n",
      "Training epoch#:  20\n",
      "errors (SSE):  [1606.68615303]\n",
      "Training epoch#:  21\n",
      "errors (SSE):  [1606.65067579]\n",
      "Training epoch#:  22\n",
      "errors (SSE):  [1606.61349327]\n",
      "Training epoch#:  23\n",
      "errors (SSE):  [1606.57438057]\n",
      "Training epoch#:  24\n",
      "errors (SSE):  [1606.53311823]\n",
      "Training epoch#:  25\n",
      "errors (SSE):  [1606.48948777]\n",
      "Training epoch#:  26\n",
      "errors (SSE):  [1606.44326814]\n",
      "Training epoch#:  27\n",
      "errors (SSE):  [1606.39423287]\n",
      "Training epoch#:  28\n",
      "errors (SSE):  [1606.3421477]\n",
      "Training epoch#:  29\n",
      "errors (SSE):  [1606.28676845]\n",
      "Training epoch#:  30\n",
      "errors (SSE):  [1606.22783922]\n",
      "Training epoch#:  31\n",
      "errors (SSE):  [1606.16509072]\n",
      "Training epoch#:  32\n",
      "errors (SSE):  [1606.09823862]\n",
      "Training epoch#:  33\n",
      "errors (SSE):  [1606.02698208]\n",
      "Training epoch#:  34\n",
      "errors (SSE):  [1605.95100229]\n",
      "Training epoch#:  35\n",
      "errors (SSE):  [1605.86996094]\n",
      "Training epoch#:  36\n",
      "errors (SSE):  [1605.78349879]\n",
      "Training epoch#:  37\n",
      "errors (SSE):  [1605.69123418]\n",
      "Training epoch#:  38\n",
      "errors (SSE):  [1605.59276151]\n",
      "Training epoch#:  39\n",
      "errors (SSE):  [1605.48764971]\n",
      "Training epoch#:  40\n",
      "errors (SSE):  [1605.37544069]\n",
      "Training epoch#:  41\n",
      "errors (SSE):  [1605.2556477]\n",
      "Training epoch#:  42\n",
      "errors (SSE):  [1605.12775369]\n",
      "Training epoch#:  43\n",
      "errors (SSE):  [1604.99120971]\n",
      "Training epoch#:  44\n",
      "errors (SSE):  [1604.84543312]\n",
      "Training epoch#:  45\n",
      "errors (SSE):  [1604.68980593]\n",
      "Training epoch#:  46\n",
      "errors (SSE):  [1604.52367306]\n",
      "Training epoch#:  47\n",
      "errors (SSE):  [1604.34634062]\n",
      "Training epoch#:  48\n",
      "errors (SSE):  [1604.15707417]\n",
      "Training epoch#:  49\n",
      "errors (SSE):  [1603.95509702]\n",
      "Training epoch#:  50\n",
      "errors (SSE):  [1603.73958863]\n",
      "Training epoch#:  51\n",
      "errors (SSE):  [1603.509683]\n",
      "Training epoch#:  52\n",
      "errors (SSE):  [1603.26446722]\n",
      "Training epoch#:  53\n",
      "errors (SSE):  [1603.00298009]\n",
      "Training epoch#:  54\n",
      "errors (SSE):  [1602.72421096]\n",
      "Training epoch#:  55\n",
      "errors (SSE):  [1602.42709872]\n",
      "Training epoch#:  56\n",
      "errors (SSE):  [1602.11053108]\n",
      "Training epoch#:  57\n",
      "errors (SSE):  [1601.77334413]\n",
      "Training epoch#:  58\n",
      "errors (SSE):  [1601.41432225]\n",
      "Training epoch#:  59\n",
      "errors (SSE):  [1601.03219841]\n",
      "Training epoch#:  60\n",
      "errors (SSE):  [1600.62565504]\n",
      "Training epoch#:  61\n",
      "errors (SSE):  [1600.19332534]\n",
      "Training epoch#:  62\n",
      "errors (SSE):  [1599.73379535]\n",
      "Training epoch#:  63\n",
      "errors (SSE):  [1599.24560665]\n",
      "Training epoch#:  64\n",
      "errors (SSE):  [1598.72725993]\n",
      "Training epoch#:  65\n",
      "errors (SSE):  [1598.17721944]\n",
      "Training epoch#:  66\n",
      "errors (SSE):  [1597.59391847]\n",
      "Training epoch#:  67\n",
      "errors (SSE):  [1596.97576595]\n",
      "Training epoch#:  68\n",
      "errors (SSE):  [1596.32115419]\n",
      "Training epoch#:  69\n",
      "errors (SSE):  [1595.62846798]\n",
      "Training epoch#:  70\n",
      "errors (SSE):  [1594.89609503]\n",
      "Training epoch#:  71\n",
      "errors (SSE):  [1594.12243787]\n",
      "Training epoch#:  72\n",
      "errors (SSE):  [1593.30592727]\n",
      "Training epoch#:  73\n",
      "errors (SSE):  [1592.44503717]\n",
      "Training epoch#:  74\n",
      "errors (SSE):  [1591.53830114]\n",
      "Training epoch#:  75\n",
      "errors (SSE):  [1590.58433042]\n",
      "Training epoch#:  76\n",
      "errors (SSE):  [1589.58183332]\n",
      "Training epoch#:  77\n",
      "errors (SSE):  [1588.52963597]\n",
      "Training epoch#:  78\n",
      "errors (SSE):  [1587.42670422]\n",
      "Training epoch#:  79\n",
      "errors (SSE):  [1586.27216644]\n",
      "Training epoch#:  80\n",
      "errors (SSE):  [1585.06533688]\n",
      "Training epoch#:  81\n",
      "errors (SSE):  [1583.80573936]\n",
      "Training epoch#:  82\n",
      "errors (SSE):  [1582.49313057]\n",
      "Training epoch#:  83\n",
      "errors (SSE):  [1581.12752286]\n",
      "Training epoch#:  84\n",
      "errors (SSE):  [1579.70920554]\n",
      "Training epoch#:  85\n",
      "errors (SSE):  [1578.23876439]\n",
      "Training epoch#:  86\n",
      "errors (SSE):  [1576.7170985]\n",
      "Training epoch#:  87\n",
      "errors (SSE):  [1575.14543399]\n",
      "Training epoch#:  88\n",
      "errors (SSE):  [1573.52533363]\n",
      "Training epoch#:  89\n",
      "errors (SSE):  [1571.85870215]\n",
      "Training epoch#:  90\n",
      "errors (SSE):  [1570.14778623]\n",
      "Training epoch#:  91\n",
      "errors (SSE):  [1568.39516912]\n",
      "Training epoch#:  92\n",
      "errors (SSE):  [1566.60375932]\n",
      "Training epoch#:  93\n",
      "errors (SSE):  [1564.77677318]\n",
      "Training epoch#:  94\n",
      "errors (SSE):  [1562.91771162]\n",
      "Training epoch#:  95\n",
      "errors (SSE):  [1561.030331]\n",
      "Training epoch#:  96\n",
      "errors (SSE):  [1559.11860866]\n",
      "Training epoch#:  97\n",
      "errors (SSE):  [1557.18670385]\n",
      "Training epoch#:  98\n",
      "errors (SSE):  [1555.23891472]\n",
      "Training epoch#:  99\n",
      "errors (SSE):  [1553.27963249]\n",
      "Training epoch#:  100\n",
      "errors (SSE):  [1551.31329385]\n",
      "Training epoch#:  101\n",
      "errors (SSE):  [1549.34433274]\n",
      "Training epoch#:  102\n",
      "errors (SSE):  [1547.3771327]\n",
      "Training epoch#:  103\n",
      "errors (SSE):  [1545.41598102]\n",
      "Training epoch#:  104\n",
      "errors (SSE):  [1543.46502563]\n",
      "Training epoch#:  105\n",
      "errors (SSE):  [1541.52823571]\n",
      "Training epoch#:  106\n",
      "errors (SSE):  [1539.60936686]\n",
      "Training epoch#:  107\n",
      "errors (SSE):  [1537.71193126]\n",
      "Training epoch#:  108\n",
      "errors (SSE):  [1535.83917339]\n",
      "Training epoch#:  109\n",
      "errors (SSE):  [1533.99405127]\n",
      "Training epoch#:  110\n",
      "errors (SSE):  [1532.17922339]\n",
      "Training epoch#:  111\n",
      "errors (SSE):  [1530.39704106]\n",
      "Training epoch#:  112\n",
      "errors (SSE):  [1528.64954592]\n",
      "Training epoch#:  113\n",
      "errors (SSE):  [1526.93847209]\n",
      "Training epoch#:  114\n",
      "errors (SSE):  [1525.26525256]\n",
      "Training epoch#:  115\n",
      "errors (SSE):  [1523.63102912]\n",
      "Training epoch#:  116\n",
      "errors (SSE):  [1522.0366654]\n",
      "Training epoch#:  117\n",
      "errors (SSE):  [1520.48276228]\n",
      "Training epoch#:  118\n",
      "errors (SSE):  [1518.96967521]\n",
      "Training epoch#:  119\n",
      "errors (SSE):  [1517.49753287]\n",
      "Training epoch#:  120\n",
      "errors (SSE):  [1516.06625663]\n",
      "Training epoch#:  121\n",
      "errors (SSE):  [1514.67558049]\n",
      "Training epoch#:  122\n",
      "errors (SSE):  [1513.32507105]\n",
      "Training epoch#:  123\n",
      "errors (SSE):  [1512.01414719]\n",
      "Training epoch#:  124\n",
      "errors (SSE):  [1510.74209918]\n",
      "Training epoch#:  125\n",
      "errors (SSE):  [1509.50810708]\n",
      "Training epoch#:  126\n",
      "errors (SSE):  [1508.31125821]\n",
      "Training epoch#:  127\n",
      "errors (SSE):  [1507.15056354]\n",
      "Training epoch#:  128\n",
      "errors (SSE):  [1506.02497298]\n",
      "Training epoch#:  129\n",
      "errors (SSE):  [1504.93338946]\n",
      "Training epoch#:  130\n",
      "errors (SSE):  [1503.87468182]\n",
      "Training epoch#:  131\n",
      "errors (SSE):  [1502.84769648]\n",
      "Training epoch#:  132\n",
      "errors (SSE):  [1501.85126789]\n",
      "Training epoch#:  133\n",
      "errors (SSE):  [1500.88422785]\n",
      "Training epoch#:  134\n",
      "errors (SSE):  [1499.94541374]\n",
      "Training epoch#:  135\n",
      "errors (SSE):  [1499.03367557]\n",
      "Training epoch#:  136\n",
      "errors (SSE):  [1498.14788218]\n",
      "Training epoch#:  137\n",
      "errors (SSE):  [1497.28692643]\n",
      "Training epoch#:  138\n",
      "errors (SSE):  [1496.4497295]\n",
      "Training epoch#:  139\n",
      "errors (SSE):  [1495.63524446]\n",
      "Training epoch#:  140\n",
      "errors (SSE):  [1494.84245909]\n",
      "Training epoch#:  141\n",
      "errors (SSE):  [1494.07039804]\n",
      "Training epoch#:  142\n",
      "errors (SSE):  [1493.31812443]\n",
      "Training epoch#:  143\n",
      "errors (SSE):  [1492.58474089]\n",
      "Training epoch#:  144\n",
      "errors (SSE):  [1491.86939021]\n",
      "Training epoch#:  145\n",
      "errors (SSE):  [1491.17125551]\n",
      "Training epoch#:  146\n",
      "errors (SSE):  [1490.48956011]\n",
      "Training epoch#:  147\n",
      "errors (SSE):  [1489.82356712]\n",
      "Training epoch#:  148\n",
      "errors (SSE):  [1489.17257875]\n",
      "Training epoch#:  149\n",
      "errors (SSE):  [1488.5359354]\n",
      "Training epoch#:  150\n",
      "errors (SSE):  [1487.91301461]\n",
      "Training epoch#:  151\n",
      "errors (SSE):  [1487.30322987]\n",
      "Training epoch#:  152\n",
      "errors (SSE):  [1486.70602933]\n",
      "Training epoch#:  153\n",
      "errors (SSE):  [1486.12089436]\n",
      "Training epoch#:  154\n",
      "errors (SSE):  [1485.54733818]\n",
      "Training epoch#:  155\n",
      "errors (SSE):  [1484.98490434]\n",
      "Training epoch#:  156\n",
      "errors (SSE):  [1484.43316524]\n",
      "Training epoch#:  157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors (SSE):  [1483.89172065]\n",
      "Training epoch#:  158\n",
      "errors (SSE):  [1483.36019617]\n",
      "Training epoch#:  159\n",
      "errors (SSE):  [1482.83824183]\n",
      "Training epoch#:  160\n",
      "errors (SSE):  [1482.32553061]\n",
      "Training epoch#:  161\n",
      "errors (SSE):  [1481.82175705]\n",
      "Training epoch#:  162\n",
      "errors (SSE):  [1481.32663588]\n",
      "Training epoch#:  163\n",
      "errors (SSE):  [1480.83990073]\n",
      "Training epoch#:  164\n",
      "errors (SSE):  [1480.36130285]\n",
      "Training epoch#:  165\n",
      "errors (SSE):  [1479.89060991]\n",
      "Training epoch#:  166\n",
      "errors (SSE):  [1479.42760482]\n",
      "Training epoch#:  167\n",
      "errors (SSE):  [1478.97208467]\n",
      "Training epoch#:  168\n",
      "errors (SSE):  [1478.52385964]\n",
      "Training epoch#:  169\n",
      "errors (SSE):  [1478.08275205]\n",
      "Training epoch#:  170\n",
      "errors (SSE):  [1477.64859538]\n",
      "Training epoch#:  171\n",
      "errors (SSE):  [1477.22123342]\n",
      "Training epoch#:  172\n",
      "errors (SSE):  [1476.80051943]\n",
      "Training epoch#:  173\n",
      "errors (SSE):  [1476.38631534]\n",
      "Training epoch#:  174\n",
      "errors (SSE):  [1475.97849106]\n",
      "Training epoch#:  175\n",
      "errors (SSE):  [1475.57692374]\n",
      "Training epoch#:  176\n",
      "errors (SSE):  [1475.1814972]\n",
      "Training epoch#:  177\n",
      "errors (SSE):  [1474.79210126]\n",
      "Training epoch#:  178\n",
      "errors (SSE):  [1474.40863125]\n",
      "Training epoch#:  179\n",
      "errors (SSE):  [1474.03098745]\n",
      "Training epoch#:  180\n",
      "errors (SSE):  [1473.65907466]\n",
      "Training epoch#:  181\n",
      "errors (SSE):  [1473.29280172]\n",
      "Training epoch#:  182\n",
      "errors (SSE):  [1472.93208111]\n",
      "Training epoch#:  183\n",
      "errors (SSE):  [1472.57682862]\n",
      "Training epoch#:  184\n",
      "errors (SSE):  [1472.22696294]\n",
      "Training epoch#:  185\n",
      "errors (SSE):  [1471.88240541]\n",
      "Training epoch#:  186\n",
      "errors (SSE):  [1471.54307968]\n",
      "Training epoch#:  187\n",
      "errors (SSE):  [1471.20891151]\n",
      "Training epoch#:  188\n",
      "errors (SSE):  [1470.87982844]\n",
      "Training epoch#:  189\n",
      "errors (SSE):  [1470.5557597]\n",
      "Training epoch#:  190\n",
      "errors (SSE):  [1470.23663588]\n",
      "Training epoch#:  191\n",
      "errors (SSE):  [1469.92238888]\n",
      "Training epoch#:  192\n",
      "errors (SSE):  [1469.61295167]\n",
      "Training epoch#:  193\n",
      "errors (SSE):  [1469.30825817]\n",
      "Training epoch#:  194\n",
      "errors (SSE):  [1469.00824316]\n",
      "Training epoch#:  195\n",
      "errors (SSE):  [1468.71284212]\n",
      "Training epoch#:  196\n",
      "errors (SSE):  [1468.42199118]\n",
      "Training epoch#:  197\n",
      "errors (SSE):  [1468.13562701]\n",
      "Training epoch#:  198\n",
      "errors (SSE):  [1467.85368677]\n",
      "Training epoch#:  199\n",
      "errors (SSE):  [1467.57610803]\n",
      "Training epoch#:  200\n",
      "errors (SSE):  [1467.30282872]\n",
      "Training epoch#:  201\n",
      "errors (SSE):  [1467.03378711]\n",
      "Training epoch#:  202\n",
      "errors (SSE):  [1466.76892175]\n",
      "Training epoch#:  203\n",
      "errors (SSE):  [1466.50817147]\n",
      "Training epoch#:  204\n",
      "errors (SSE):  [1466.25147534]\n",
      "Training epoch#:  205\n",
      "errors (SSE):  [1465.99877267]\n",
      "Training epoch#:  206\n",
      "errors (SSE):  [1465.750003]\n",
      "Training epoch#:  207\n",
      "errors (SSE):  [1465.50510609]\n",
      "Training epoch#:  208\n",
      "errors (SSE):  [1465.26402192]\n",
      "Training epoch#:  209\n",
      "errors (SSE):  [1465.02669073]\n",
      "Training epoch#:  210\n",
      "errors (SSE):  [1464.79305299]\n",
      "Training epoch#:  211\n",
      "errors (SSE):  [1464.5630494]\n",
      "Training epoch#:  212\n",
      "errors (SSE):  [1464.33662098]\n",
      "Training epoch#:  213\n",
      "errors (SSE):  [1464.11370898]\n",
      "Training epoch#:  214\n",
      "errors (SSE):  [1463.89425501]\n",
      "Training epoch#:  215\n",
      "errors (SSE):  [1463.67820097]\n",
      "Training epoch#:  216\n",
      "errors (SSE):  [1463.46548911]\n",
      "Training epoch#:  217\n",
      "errors (SSE):  [1463.25606207]\n",
      "Training epoch#:  218\n",
      "errors (SSE):  [1463.04986286]\n",
      "Training epoch#:  219\n",
      "errors (SSE):  [1462.84683492]\n",
      "Training epoch#:  220\n",
      "errors (SSE):  [1462.64692214]\n",
      "Training epoch#:  221\n",
      "errors (SSE):  [1462.45006884]\n",
      "Training epoch#:  222\n",
      "errors (SSE):  [1462.25621987]\n",
      "Training epoch#:  223\n",
      "errors (SSE):  [1462.06532056]\n",
      "Training epoch#:  224\n",
      "errors (SSE):  [1461.87731677]\n",
      "Training epoch#:  225\n",
      "errors (SSE):  [1461.69215496]\n",
      "Training epoch#:  226\n",
      "errors (SSE):  [1461.50978211]\n",
      "Training epoch#:  227\n",
      "errors (SSE):  [1461.33014585]\n",
      "Training epoch#:  228\n",
      "errors (SSE):  [1461.15319439]\n",
      "Training epoch#:  229\n",
      "errors (SSE):  [1460.9788766]\n",
      "Training epoch#:  230\n",
      "errors (SSE):  [1460.807142]\n",
      "Training epoch#:  231\n",
      "errors (SSE):  [1460.63794079]\n",
      "Training epoch#:  232\n",
      "errors (SSE):  [1460.47122386]\n",
      "Training epoch#:  233\n",
      "errors (SSE):  [1460.30694278]\n",
      "Training epoch#:  234\n",
      "errors (SSE):  [1460.14504988]\n",
      "Training epoch#:  235\n",
      "errors (SSE):  [1459.98549818]\n",
      "Training epoch#:  236\n",
      "errors (SSE):  [1459.82824148]\n",
      "Training epoch#:  237\n",
      "errors (SSE):  [1459.6732343]\n",
      "Training epoch#:  238\n",
      "errors (SSE):  [1459.52043195]\n",
      "Training epoch#:  239\n",
      "errors (SSE):  [1459.3697905]\n",
      "Training epoch#:  240\n",
      "errors (SSE):  [1459.22126678]\n",
      "Training epoch#:  241\n",
      "errors (SSE):  [1459.07481844]\n",
      "Training epoch#:  242\n",
      "errors (SSE):  [1458.93040389]\n",
      "Training epoch#:  243\n",
      "errors (SSE):  [1458.78798233]\n",
      "Training epoch#:  244\n",
      "errors (SSE):  [1458.64751377]\n",
      "Training epoch#:  245\n",
      "errors (SSE):  [1458.50895901]\n",
      "Training epoch#:  246\n",
      "errors (SSE):  [1458.37227964]\n",
      "Training epoch#:  247\n",
      "errors (SSE):  [1458.23743806]\n",
      "Training epoch#:  248\n",
      "errors (SSE):  [1458.10439743]\n",
      "Training epoch#:  249\n",
      "errors (SSE):  [1457.97312175]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "ann = neuralNetwork(datasets['wine']['inputnodes'],\n",
    "                   datasets['wine']['hiddennodes'],\n",
    "                   datasets['wine']['outputnodes'],\n",
    "                   datasets['wine']['lr'],\n",
    "                   datasets['wine']['bs'],\n",
    "                   datasets['wine']['epochs'])\n",
    "ann.train(datasets['wine']['train'])\n",
    "#Test\n",
    "correct = 0\n",
    "ann.test(datasets['wine']['test'])\n",
    "for result in ann.results:\n",
    "    if (result[0] == result[1]):\n",
    "        correct += 1\n",
    "    pass \n",
    "correct = 100 * (correct/len(ann.results))\n",
    "wine_results.append(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Breast Cancer Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch#:  0\n",
      "errors (SSE):  [142.18884967]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [141.22213423]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [140.40052493]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [139.25089155]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [137.54183342]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [134.93280465]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [130.96058675]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [125.09904001]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [116.96956739]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [106.69739367]\n",
      "Training epoch#:  10\n",
      "errors (SSE):  [95.13855649]\n",
      "Training epoch#:  11\n",
      "errors (SSE):  [83.59155528]\n",
      "Training epoch#:  12\n",
      "errors (SSE):  [73.15733092]\n",
      "Training epoch#:  13\n",
      "errors (SSE):  [64.36072851]\n",
      "Training epoch#:  14\n",
      "errors (SSE):  [57.22052011]\n",
      "Training epoch#:  15\n",
      "errors (SSE):  [51.50559941]\n",
      "Training epoch#:  16\n",
      "errors (SSE):  [46.93146365]\n",
      "Training epoch#:  17\n",
      "errors (SSE):  [43.24444532]\n",
      "Training epoch#:  18\n",
      "errors (SSE):  [40.24101907]\n",
      "Training epoch#:  19\n",
      "errors (SSE):  [37.76405457]\n",
      "Training epoch#:  20\n",
      "errors (SSE):  [35.69392818]\n",
      "Training epoch#:  21\n",
      "errors (SSE):  [33.94001056]\n",
      "Training epoch#:  22\n",
      "errors (SSE):  [32.43371431]\n",
      "Training epoch#:  23\n",
      "errors (SSE):  [31.12307703]\n",
      "Training epoch#:  24\n",
      "errors (SSE):  [29.96861974]\n",
      "Training epoch#:  25\n",
      "errors (SSE):  [28.94021831]\n",
      "Training epoch#:  26\n",
      "errors (SSE):  [28.0147607]\n",
      "Training epoch#:  27\n",
      "errors (SSE):  [27.17440154]\n",
      "Training epoch#:  28\n",
      "errors (SSE):  [26.40526383]\n",
      "Training epoch#:  29\n",
      "errors (SSE):  [25.69647242]\n",
      "Training epoch#:  30\n",
      "errors (SSE):  [25.039433]\n",
      "Training epoch#:  31\n",
      "errors (SSE):  [24.42729257]\n",
      "Training epoch#:  32\n",
      "errors (SSE):  [23.85453456]\n",
      "Training epoch#:  33\n",
      "errors (SSE):  [23.31667392]\n",
      "Training epoch#:  34\n",
      "errors (SSE):  [22.81002667]\n",
      "Training epoch#:  35\n",
      "errors (SSE):  [22.33153516]\n",
      "Training epoch#:  36\n",
      "errors (SSE):  [21.87863493]\n",
      "Training epoch#:  37\n",
      "errors (SSE):  [21.44915285]\n",
      "Training epoch#:  38\n",
      "errors (SSE):  [21.04122891]\n",
      "Training epoch#:  39\n",
      "errors (SSE):  [20.65325571]\n",
      "Training epoch#:  40\n",
      "errors (SSE):  [20.28383153]\n",
      "Training epoch#:  41\n",
      "errors (SSE):  [19.93172358]\n",
      "Training epoch#:  42\n",
      "errors (SSE):  [19.59583915]\n",
      "Training epoch#:  43\n",
      "errors (SSE):  [19.27520269]\n",
      "Training epoch#:  44\n",
      "errors (SSE):  [18.96893756]\n",
      "Training epoch#:  45\n",
      "errors (SSE):  [18.67625125]\n",
      "Training epoch#:  46\n",
      "errors (SSE):  [18.3964235]\n",
      "Training epoch#:  47\n",
      "errors (SSE):  [18.12879641]\n",
      "Training epoch#:  48\n",
      "errors (SSE):  [17.87276637]\n",
      "Training epoch#:  49\n",
      "errors (SSE):  [17.62777726]\n",
      "Training epoch#:  50\n",
      "errors (SSE):  [17.39331473]\n",
      "Training epoch#:  51\n",
      "errors (SSE):  [17.16890137]\n",
      "Training epoch#:  52\n",
      "errors (SSE):  [16.95409252]\n",
      "Training epoch#:  53\n",
      "errors (SSE):  [16.74847274]\n",
      "Training epoch#:  54\n",
      "errors (SSE):  [16.55165268]\n",
      "Training epoch#:  55\n",
      "errors (SSE):  [16.36326644]\n",
      "Training epoch#:  56\n",
      "errors (SSE):  [16.18296918]\n",
      "Training epoch#:  57\n",
      "errors (SSE):  [16.01043515]\n",
      "Training epoch#:  58\n",
      "errors (SSE):  [15.84535585]\n",
      "Training epoch#:  59\n",
      "errors (SSE):  [15.68743854]\n",
      "Training epoch#:  60\n",
      "errors (SSE):  [15.53640484]\n",
      "Training epoch#:  61\n",
      "errors (SSE):  [15.3919896]\n",
      "Training epoch#:  62\n",
      "errors (SSE):  [15.25393979]\n",
      "Training epoch#:  63\n",
      "errors (SSE):  [15.12201367]\n",
      "Training epoch#:  64\n",
      "errors (SSE):  [14.99597997]\n",
      "Training epoch#:  65\n",
      "errors (SSE):  [14.87561718]\n",
      "Training epoch#:  66\n",
      "errors (SSE):  [14.76071295]\n",
      "Training epoch#:  67\n",
      "errors (SSE):  [14.65106355]\n",
      "Training epoch#:  68\n",
      "errors (SSE):  [14.54647338]\n",
      "Training epoch#:  69\n",
      "errors (SSE):  [14.44675452]\n",
      "Training epoch#:  70\n",
      "errors (SSE):  [14.35172639]\n",
      "Training epoch#:  71\n",
      "errors (SSE):  [14.26121534]\n",
      "Training epoch#:  72\n",
      "errors (SSE):  [14.17505436]\n",
      "Training epoch#:  73\n",
      "errors (SSE):  [14.09308278]\n",
      "Training epoch#:  74\n",
      "errors (SSE):  [14.01514599]\n",
      "Training epoch#:  75\n",
      "errors (SSE):  [13.94109518]\n",
      "Training epoch#:  76\n",
      "errors (SSE):  [13.87078714]\n",
      "Training epoch#:  77\n",
      "errors (SSE):  [13.80408396]\n",
      "Training epoch#:  78\n",
      "errors (SSE):  [13.74085291]\n",
      "Training epoch#:  79\n",
      "errors (SSE):  [13.68096614]\n",
      "Training epoch#:  80\n",
      "errors (SSE):  [13.62430057]\n",
      "Training epoch#:  81\n",
      "errors (SSE):  [13.57073766]\n",
      "Training epoch#:  82\n",
      "errors (SSE):  [13.52016322]\n",
      "Training epoch#:  83\n",
      "errors (SSE):  [13.47246727]\n",
      "Training epoch#:  84\n",
      "errors (SSE):  [13.42754385]\n",
      "Training epoch#:  85\n",
      "errors (SSE):  [13.38529086]\n",
      "Training epoch#:  86\n",
      "errors (SSE):  [13.34560992]\n",
      "Training epoch#:  87\n",
      "errors (SSE):  [13.30840618]\n",
      "Training epoch#:  88\n",
      "errors (SSE):  [13.27358819]\n",
      "Training epoch#:  89\n",
      "errors (SSE):  [13.24106778]\n",
      "Training epoch#:  90\n",
      "errors (SSE):  [13.21075987]\n",
      "Training epoch#:  91\n",
      "errors (SSE):  [13.18258238]\n",
      "Training epoch#:  92\n",
      "errors (SSE):  [13.15645605]\n",
      "Training epoch#:  93\n",
      "errors (SSE):  [13.13230438]\n",
      "Training epoch#:  94\n",
      "errors (SSE):  [13.11005342]\n",
      "Training epoch#:  95\n",
      "errors (SSE):  [13.08963173]\n",
      "Training epoch#:  96\n",
      "errors (SSE):  [13.07097022]\n",
      "Training epoch#:  97\n",
      "errors (SSE):  [13.05400205]\n",
      "Training epoch#:  98\n",
      "errors (SSE):  [13.03866253]\n",
      "Training epoch#:  99\n",
      "errors (SSE):  [13.02488901]\n",
      "Training epoch#:  100\n",
      "errors (SSE):  [13.01262079]\n",
      "Training epoch#:  101\n",
      "errors (SSE):  [13.00179903]\n",
      "Training epoch#:  102\n",
      "errors (SSE):  [12.99236665]\n",
      "Training epoch#:  103\n",
      "errors (SSE):  [12.98426827]\n",
      "Training epoch#:  104\n",
      "errors (SSE):  [12.97745009]\n",
      "Training epoch#:  105\n",
      "errors (SSE):  [12.97185988]\n",
      "Training epoch#:  106\n",
      "errors (SSE):  [12.96744684]\n",
      "Training epoch#:  107\n",
      "errors (SSE):  [12.96416159]\n",
      "Training epoch#:  108\n",
      "errors (SSE):  [12.96195608]\n",
      "Training epoch#:  109\n",
      "errors (SSE):  [12.96078353]\n",
      "Training epoch#:  110\n",
      "errors (SSE):  [12.9605984]\n",
      "Training epoch#:  111\n",
      "errors (SSE):  [12.96135632]\n",
      "Training epoch#:  112\n",
      "errors (SSE):  [12.96301408]\n",
      "Training epoch#:  113\n",
      "errors (SSE):  [12.96552952]\n",
      "Training epoch#:  114\n",
      "errors (SSE):  [12.96886157]\n",
      "Training epoch#:  115\n",
      "errors (SSE):  [12.97297017]\n",
      "Training epoch#:  116\n",
      "errors (SSE):  [12.97781624]\n",
      "Training epoch#:  117\n",
      "errors (SSE):  [12.98336169]\n",
      "Training epoch#:  118\n",
      "errors (SSE):  [12.98956933]\n",
      "Training epoch#:  119\n",
      "errors (SSE):  [12.99640293]\n",
      "Training epoch#:  120\n",
      "errors (SSE):  [13.00382712]\n",
      "Training epoch#:  121\n",
      "errors (SSE):  [13.01180742]\n",
      "Training epoch#:  122\n",
      "errors (SSE):  [13.02031024]\n",
      "Training epoch#:  123\n",
      "errors (SSE):  [13.0293028]\n",
      "Training epoch#:  124\n",
      "errors (SSE):  [13.03875318]\n",
      "Training epoch#:  125\n",
      "errors (SSE):  [13.04863032]\n",
      "Training epoch#:  126\n",
      "errors (SSE):  [13.05890393]\n",
      "Training epoch#:  127\n",
      "errors (SSE):  [13.06954458]\n",
      "Training epoch#:  128\n",
      "errors (SSE):  [13.08052363]\n",
      "Training epoch#:  129\n",
      "errors (SSE):  [13.09181326]\n",
      "Training epoch#:  130\n",
      "errors (SSE):  [13.10338644]\n",
      "Training epoch#:  131\n",
      "errors (SSE):  [13.11521694]\n",
      "Training epoch#:  132\n",
      "errors (SSE):  [13.12727933]\n",
      "Training epoch#:  133\n",
      "errors (SSE):  [13.13954898]\n",
      "Training epoch#:  134\n",
      "errors (SSE):  [13.15200203]\n",
      "Training epoch#:  135\n",
      "errors (SSE):  [13.16461541]\n",
      "Training epoch#:  136\n",
      "errors (SSE):  [13.17736683]\n",
      "Training epoch#:  137\n",
      "errors (SSE):  [13.1902348]\n",
      "Training epoch#:  138\n",
      "errors (SSE):  [13.20319858]\n",
      "Training epoch#:  139\n",
      "errors (SSE):  [13.21623821]\n",
      "Training epoch#:  140\n",
      "errors (SSE):  [13.22933447]\n",
      "Training epoch#:  141\n",
      "errors (SSE):  [13.24246892]\n",
      "Training epoch#:  142\n",
      "errors (SSE):  [13.25562388]\n",
      "Training epoch#:  143\n",
      "errors (SSE):  [13.26878239]\n",
      "Training epoch#:  144\n",
      "errors (SSE):  [13.28192823]\n",
      "Training epoch#:  145\n",
      "errors (SSE):  [13.29504591]\n",
      "Training epoch#:  146\n",
      "errors (SSE):  [13.30812065]\n",
      "Training epoch#:  147\n",
      "errors (SSE):  [13.32113839]\n",
      "Training epoch#:  148\n",
      "errors (SSE):  [13.33408575]\n",
      "Training epoch#:  149\n",
      "errors (SSE):  [13.34695004]\n",
      "Training epoch#:  150\n",
      "errors (SSE):  [13.35971924]\n",
      "Training epoch#:  151\n",
      "errors (SSE):  [13.37238198]\n",
      "Training epoch#:  152\n",
      "errors (SSE):  [13.38492755]\n",
      "Training epoch#:  153\n",
      "errors (SSE):  [13.39734585]\n",
      "Training epoch#:  154\n",
      "errors (SSE):  [13.40962741]\n",
      "Training epoch#:  155\n",
      "errors (SSE):  [13.42176335]\n",
      "Training epoch#:  156\n",
      "errors (SSE):  [13.43374538]\n",
      "Training epoch#:  157\n",
      "errors (SSE):  [13.44556578]\n",
      "Training epoch#:  158\n",
      "errors (SSE):  [13.45721738]\n",
      "Training epoch#:  159\n",
      "errors (SSE):  [13.46869353]\n",
      "Training epoch#:  160\n",
      "errors (SSE):  [13.47998814]\n",
      "Training epoch#:  161\n",
      "errors (SSE):  [13.49109558]\n",
      "Training epoch#:  162\n",
      "errors (SSE):  [13.50201072]\n",
      "Training epoch#:  163\n",
      "errors (SSE):  [13.51272892]\n",
      "Training epoch#:  164\n",
      "errors (SSE):  [13.52324597]\n",
      "Training epoch#:  165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors (SSE):  [13.53355809]\n",
      "Training epoch#:  166\n",
      "errors (SSE):  [13.54366196]\n",
      "Training epoch#:  167\n",
      "errors (SSE):  [13.55355461]\n",
      "Training epoch#:  168\n",
      "errors (SSE):  [13.5632335]\n",
      "Training epoch#:  169\n",
      "errors (SSE):  [13.57269644]\n",
      "Training epoch#:  170\n",
      "errors (SSE):  [13.5819416]\n",
      "Training epoch#:  171\n",
      "errors (SSE):  [13.59096749]\n",
      "Training epoch#:  172\n",
      "errors (SSE):  [13.59977296]\n",
      "Training epoch#:  173\n",
      "errors (SSE):  [13.60835714]\n",
      "Training epoch#:  174\n",
      "errors (SSE):  [13.61671948]\n",
      "Training epoch#:  175\n",
      "errors (SSE):  [13.6248597]\n",
      "Training epoch#:  176\n",
      "errors (SSE):  [13.6327778]\n",
      "Training epoch#:  177\n",
      "errors (SSE):  [13.64047402]\n",
      "Training epoch#:  178\n",
      "errors (SSE):  [13.64794885]\n",
      "Training epoch#:  179\n",
      "errors (SSE):  [13.65520301]\n",
      "Training epoch#:  180\n",
      "errors (SSE):  [13.66223742]\n",
      "Training epoch#:  181\n",
      "errors (SSE):  [13.66905322]\n",
      "Training epoch#:  182\n",
      "errors (SSE):  [13.67565173]\n",
      "Training epoch#:  183\n",
      "errors (SSE):  [13.68203447]\n",
      "Training epoch#:  184\n",
      "errors (SSE):  [13.68820311]\n",
      "Training epoch#:  185\n",
      "errors (SSE):  [13.69415948]\n",
      "Training epoch#:  186\n",
      "errors (SSE):  [13.69990558]\n",
      "Training epoch#:  187\n",
      "errors (SSE):  [13.70544353]\n",
      "Training epoch#:  188\n",
      "errors (SSE):  [13.71077558]\n",
      "Training epoch#:  189\n",
      "errors (SSE):  [13.71590411]\n",
      "Training epoch#:  190\n",
      "errors (SSE):  [13.7208316]\n",
      "Training epoch#:  191\n",
      "errors (SSE):  [13.72556065]\n",
      "Training epoch#:  192\n",
      "errors (SSE):  [13.73009394]\n",
      "Training epoch#:  193\n",
      "errors (SSE):  [13.73443424]\n",
      "Training epoch#:  194\n",
      "errors (SSE):  [13.7385844]\n",
      "Training epoch#:  195\n",
      "errors (SSE):  [13.74254736]\n",
      "Training epoch#:  196\n",
      "errors (SSE):  [13.74632609]\n",
      "Training epoch#:  197\n",
      "errors (SSE):  [13.74992366]\n",
      "Training epoch#:  198\n",
      "errors (SSE):  [13.75334315]\n",
      "Training epoch#:  199\n",
      "errors (SSE):  [13.75658773]\n",
      "Training epoch#:  200\n",
      "errors (SSE):  [13.75966057]\n",
      "Training epoch#:  201\n",
      "errors (SSE):  [13.76256491]\n",
      "Training epoch#:  202\n",
      "errors (SSE):  [13.765304]\n",
      "Training epoch#:  203\n",
      "errors (SSE):  [13.76788112]\n",
      "Training epoch#:  204\n",
      "errors (SSE):  [13.77029956]\n",
      "Training epoch#:  205\n",
      "errors (SSE):  [13.77256263]\n",
      "Training epoch#:  206\n",
      "errors (SSE):  [13.77467368]\n",
      "Training epoch#:  207\n",
      "errors (SSE):  [13.77663602]\n",
      "Training epoch#:  208\n",
      "errors (SSE):  [13.77845298]\n",
      "Training epoch#:  209\n",
      "errors (SSE):  [13.78012791]\n",
      "Training epoch#:  210\n",
      "errors (SSE):  [13.78166413]\n",
      "Training epoch#:  211\n",
      "errors (SSE):  [13.78306496]\n",
      "Training epoch#:  212\n",
      "errors (SSE):  [13.7843337]\n",
      "Training epoch#:  213\n",
      "errors (SSE):  [13.78547365]\n",
      "Training epoch#:  214\n",
      "errors (SSE):  [13.78648808]\n",
      "Training epoch#:  215\n",
      "errors (SSE):  [13.78738025]\n",
      "Training epoch#:  216\n",
      "errors (SSE):  [13.7881534]\n",
      "Training epoch#:  217\n",
      "errors (SSE):  [13.78881072]\n",
      "Training epoch#:  218\n",
      "errors (SSE):  [13.7893554]\n",
      "Training epoch#:  219\n",
      "errors (SSE):  [13.78979059]\n",
      "Training epoch#:  220\n",
      "errors (SSE):  [13.7901194]\n",
      "Training epoch#:  221\n",
      "errors (SSE):  [13.79034492]\n",
      "Training epoch#:  222\n",
      "errors (SSE):  [13.79047019]\n",
      "Training epoch#:  223\n",
      "errors (SSE):  [13.79049823]\n",
      "Training epoch#:  224\n",
      "errors (SSE):  [13.790432]\n",
      "Training epoch#:  225\n",
      "errors (SSE):  [13.79027445]\n",
      "Training epoch#:  226\n",
      "errors (SSE):  [13.79002845]\n",
      "Training epoch#:  227\n",
      "errors (SSE):  [13.78969686]\n",
      "Training epoch#:  228\n",
      "errors (SSE):  [13.78928248]\n",
      "Training epoch#:  229\n",
      "errors (SSE):  [13.78878807]\n",
      "Training epoch#:  230\n",
      "errors (SSE):  [13.78821634]\n",
      "Training epoch#:  231\n",
      "errors (SSE):  [13.78756996]\n",
      "Training epoch#:  232\n",
      "errors (SSE):  [13.78685155]\n",
      "Training epoch#:  233\n",
      "errors (SSE):  [13.78606368]\n",
      "Training epoch#:  234\n",
      "errors (SSE):  [13.78520886]\n",
      "Training epoch#:  235\n",
      "errors (SSE):  [13.78428958]\n",
      "Training epoch#:  236\n",
      "errors (SSE):  [13.78330826]\n",
      "Training epoch#:  237\n",
      "errors (SSE):  [13.78226727]\n",
      "Training epoch#:  238\n",
      "errors (SSE):  [13.78116894]\n",
      "Training epoch#:  239\n",
      "errors (SSE):  [13.78001554]\n",
      "Training epoch#:  240\n",
      "errors (SSE):  [13.77880929]\n",
      "Training epoch#:  241\n",
      "errors (SSE):  [13.77755236]\n",
      "Training epoch#:  242\n",
      "errors (SSE):  [13.77624688]\n",
      "Training epoch#:  243\n",
      "errors (SSE):  [13.77489492]\n",
      "Training epoch#:  244\n",
      "errors (SSE):  [13.77349849]\n",
      "Training epoch#:  245\n",
      "errors (SSE):  [13.77205957]\n",
      "Training epoch#:  246\n",
      "errors (SSE):  [13.77058009]\n",
      "Training epoch#:  247\n",
      "errors (SSE):  [13.7690619]\n",
      "Training epoch#:  248\n",
      "errors (SSE):  [13.76750682]\n",
      "Training epoch#:  249\n",
      "errors (SSE):  [13.76591664]\n",
      "Training epoch#:  250\n",
      "errors (SSE):  [13.76429306]\n",
      "Training epoch#:  251\n",
      "errors (SSE):  [13.76263777]\n",
      "Training epoch#:  252\n",
      "errors (SSE):  [13.76095239]\n",
      "Training epoch#:  253\n",
      "errors (SSE):  [13.75923848]\n",
      "Training epoch#:  254\n",
      "errors (SSE):  [13.75749759]\n",
      "Training epoch#:  255\n",
      "errors (SSE):  [13.75573119]\n",
      "Training epoch#:  256\n",
      "errors (SSE):  [13.75394072]\n",
      "Training epoch#:  257\n",
      "errors (SSE):  [13.75212757]\n",
      "Training epoch#:  258\n",
      "errors (SSE):  [13.75029307]\n",
      "Training epoch#:  259\n",
      "errors (SSE):  [13.74843855]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "ann = neuralNetwork(datasets['breast_cancer']['inputnodes'],\n",
    "                   datasets['breast_cancer']['hiddennodes'],\n",
    "                   datasets['breast_cancer']['outputnodes'],\n",
    "                   datasets['breast_cancer']['lr'],\n",
    "                   datasets['breast_cancer']['bs'],\n",
    "                   datasets['breast_cancer']['epochs'])\n",
    "ann.train(datasets['breast_cancer']['train'])\n",
    "#Test\n",
    "correct = 0\n",
    "ann.test(datasets['breast_cancer']['test'])\n",
    "for result in ann.results:\n",
    "    if (result[0] == result[1]):\n",
    "        correct += 1\n",
    "    pass \n",
    "correct = 100 * (correct/len(ann.results))\n",
    "breast_cancer_results.append(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_weighted = [False, True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wine Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test\n",
    "knn = kNN(datasets['wine']['train_x'].values,\n",
    "          datasets['wine']['train_y'].values,\n",
    "          datasets['wine']['k'],\n",
    "         weighted=datasets['wine']['weighted'])\n",
    "knn.test(datasets['wine']['test_x'].values,datasets['wine']['test_y'].values)\n",
    "wine_results.append(accuracy(knn.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Breast Cancer Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test\n",
    "knn = kNN(datasets['breast_cancer']['train_x'].values,\n",
    "          datasets['breast_cancer']['train_y'].values,\n",
    "          datasets['breast_cancer']['k'],\n",
    "         weighted=datasets['breast_cancer']['weighted'])\n",
    "knn.test(datasets['breast_cancer']['test_x'].values,datasets['breast_cancer']['test_y'].values)\n",
    "breast_cancer_results.append(accuracy(knn.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN-kNN Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch#:  0\n",
      "errors (SSE):  [1726.49855323]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [1611.75526924]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [1609.05312021]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [1608.14050778]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [1607.69911051]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [1607.44368899]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [1607.27758919]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [1607.15958132]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [1607.06940106]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [1606.9960674]\n",
      "Training epoch#:  10\n",
      "errors (SSE):  [1606.93317753]\n",
      "Training epoch#:  11\n",
      "errors (SSE):  [1606.87677164]\n",
      "Training epoch#:  12\n",
      "errors (SSE):  [1606.82427454]\n",
      "Training epoch#:  13\n",
      "errors (SSE):  [1606.77393397]\n",
      "Training epoch#:  14\n",
      "errors (SSE):  [1606.72450536]\n",
      "Training epoch#:  15\n",
      "errors (SSE):  [1606.67506666]\n",
      "Training epoch#:  16\n",
      "errors (SSE):  [1606.62490529]\n",
      "Training epoch#:  17\n",
      "errors (SSE):  [1606.57344656]\n",
      "Training epoch#:  18\n",
      "errors (SSE):  [1606.52020717]\n",
      "Training epoch#:  19\n",
      "errors (SSE):  [1606.46476392]\n",
      "Training epoch#:  20\n",
      "errors (SSE):  [1606.40673232]\n",
      "Training epoch#:  21\n",
      "errors (SSE):  [1606.3457514]\n",
      "Training epoch#:  22\n",
      "errors (SSE):  [1606.28147278]\n",
      "Training epoch#:  23\n",
      "errors (SSE):  [1606.21355252]\n",
      "Training epoch#:  24\n",
      "errors (SSE):  [1606.14164489]\n",
      "Training epoch#:  25\n",
      "errors (SSE):  [1606.06539746]\n",
      "Training epoch#:  26\n",
      "errors (SSE):  [1605.98444715]\n",
      "Training epoch#:  27\n",
      "errors (SSE):  [1605.89841687]\n",
      "Training epoch#:  28\n",
      "errors (SSE):  [1605.80691264]\n",
      "Training epoch#:  29\n",
      "errors (SSE):  [1605.70952104]\n",
      "Training epoch#:  30\n",
      "errors (SSE):  [1605.60580685]\n",
      "Training epoch#:  31\n",
      "errors (SSE):  [1605.49531088]\n",
      "Training epoch#:  32\n",
      "errors (SSE):  [1605.37754785]\n",
      "Training epoch#:  33\n",
      "errors (SSE):  [1605.25200439]\n",
      "Training epoch#:  34\n",
      "errors (SSE):  [1605.11813705]\n",
      "Training epoch#:  35\n",
      "errors (SSE):  [1604.97537035]\n",
      "Training epoch#:  36\n",
      "errors (SSE):  [1604.82309488]\n",
      "Training epoch#:  37\n",
      "errors (SSE):  [1604.66066534]\n",
      "Training epoch#:  38\n",
      "errors (SSE):  [1604.48739867]\n",
      "Training epoch#:  39\n",
      "errors (SSE):  [1604.30257218]\n",
      "Training epoch#:  40\n",
      "errors (SSE):  [1604.1054217]\n",
      "Training epoch#:  41\n",
      "errors (SSE):  [1603.8951398]\n",
      "Training epoch#:  42\n",
      "errors (SSE):  [1603.67087406]\n",
      "Training epoch#:  43\n",
      "errors (SSE):  [1603.43172541]\n",
      "Training epoch#:  44\n",
      "errors (SSE):  [1603.17674663]\n",
      "Training epoch#:  45\n",
      "errors (SSE):  [1602.90494097]\n",
      "Training epoch#:  46\n",
      "errors (SSE):  [1602.61526091]\n",
      "Training epoch#:  47\n",
      "errors (SSE):  [1602.30660723]\n",
      "Training epoch#:  48\n",
      "errors (SSE):  [1601.97782828]\n",
      "Training epoch#:  49\n",
      "errors (SSE):  [1601.62771959]\n",
      "Training epoch#:  50\n",
      "errors (SSE):  [1601.25502389]\n",
      "Training epoch#:  51\n",
      "errors (SSE):  [1600.85843154]\n",
      "Training epoch#:  52\n",
      "errors (SSE):  [1600.43658152]\n",
      "Training epoch#:  53\n",
      "errors (SSE):  [1599.98806297]\n",
      "Training epoch#:  54\n",
      "errors (SSE):  [1599.51141747]\n",
      "Training epoch#:  55\n",
      "errors (SSE):  [1599.00514206]\n",
      "Training epoch#:  56\n",
      "errors (SSE):  [1598.46769317]\n",
      "Training epoch#:  57\n",
      "errors (SSE):  [1597.89749143]\n",
      "Training epoch#:  58\n",
      "errors (SSE):  [1597.29292769]\n",
      "Training epoch#:  59\n",
      "errors (SSE):  [1596.65237007]\n",
      "Training epoch#:  60\n",
      "errors (SSE):  [1595.97417241]\n",
      "Training epoch#:  61\n",
      "errors (SSE):  [1595.25668399]\n",
      "Training epoch#:  62\n",
      "errors (SSE):  [1594.49826077]\n",
      "Training epoch#:  63\n",
      "errors (SSE):  [1593.69727808]\n",
      "Training epoch#:  64\n",
      "errors (SSE):  [1592.852145]\n",
      "Training epoch#:  65\n",
      "errors (SSE):  [1591.96132022]\n",
      "Training epoch#:  66\n",
      "errors (SSE):  [1591.02332958]\n",
      "Training epoch#:  67\n",
      "errors (SSE):  [1590.03678513]\n",
      "Training epoch#:  68\n",
      "errors (SSE):  [1589.00040568]\n",
      "Training epoch#:  69\n",
      "errors (SSE):  [1587.91303867]\n",
      "Training epoch#:  70\n",
      "errors (SSE):  [1586.77368313]\n",
      "Training epoch#:  71\n",
      "errors (SSE):  [1585.58151361]\n",
      "Training epoch#:  72\n",
      "errors (SSE):  [1584.33590444]\n",
      "Training epoch#:  73\n",
      "errors (SSE):  [1583.03645434]\n",
      "Training epoch#:  74\n",
      "errors (SSE):  [1581.68301042]\n",
      "Training epoch#:  75\n",
      "errors (SSE):  [1580.27569145]\n",
      "Training epoch#:  76\n",
      "errors (SSE):  [1578.81490943]\n",
      "Training epoch#:  77\n",
      "errors (SSE):  [1577.30138906]\n",
      "Training epoch#:  78\n",
      "errors (SSE):  [1575.73618425]\n",
      "Training epoch#:  79\n",
      "errors (SSE):  [1574.12069099]\n",
      "Training epoch#:  80\n",
      "errors (SSE):  [1572.45665601]\n",
      "Training epoch#:  81\n",
      "errors (SSE):  [1570.7461804]\n",
      "Training epoch#:  82\n",
      "errors (SSE):  [1568.99171778]\n",
      "Training epoch#:  83\n",
      "errors (SSE):  [1567.19606649]\n",
      "Training epoch#:  84\n",
      "errors (SSE):  [1565.36235563]\n",
      "Training epoch#:  85\n",
      "errors (SSE):  [1563.49402466]\n",
      "Training epoch#:  86\n",
      "errors (SSE):  [1561.59479686]\n",
      "Training epoch#:  87\n",
      "errors (SSE):  [1559.66864683]\n",
      "Training epoch#:  88\n",
      "errors (SSE):  [1557.71976257]\n",
      "Training epoch#:  89\n",
      "errors (SSE):  [1555.75250303]\n",
      "Training epoch#:  90\n",
      "errors (SSE):  [1553.77135181]\n",
      "Training epoch#:  91\n",
      "errors (SSE):  [1551.7808684]\n",
      "Training epoch#:  92\n",
      "errors (SSE):  [1549.78563792]\n",
      "Training epoch#:  93\n",
      "errors (SSE):  [1547.79022065]\n",
      "Training epoch#:  94\n",
      "errors (SSE):  [1545.79910279]\n",
      "Training epoch#:  95\n",
      "errors (SSE):  [1543.81664934]\n",
      "Training epoch#:  96\n",
      "errors (SSE):  [1541.84706041]\n",
      "Training epoch#:  97\n",
      "errors (SSE):  [1539.89433182]\n",
      "Training epoch#:  98\n",
      "errors (SSE):  [1537.96222071]\n",
      "Training epoch#:  99\n",
      "errors (SSE):  [1536.05421673]\n",
      "Training epoch#:  100\n",
      "errors (SSE):  [1534.17351906]\n",
      "Training epoch#:  101\n",
      "errors (SSE):  [1532.32301955]\n",
      "Training epoch#:  102\n",
      "errors (SSE):  [1530.50529161]\n",
      "Training epoch#:  103\n",
      "errors (SSE):  [1528.72258492]\n",
      "Training epoch#:  104\n",
      "errors (SSE):  [1526.97682533]\n",
      "Training epoch#:  105\n",
      "errors (SSE):  [1525.26961951]\n",
      "Training epoch#:  106\n",
      "errors (SSE):  [1523.60226382]\n",
      "Training epoch#:  107\n",
      "errors (SSE):  [1521.97575677]\n",
      "Training epoch#:  108\n",
      "errors (SSE):  [1520.39081433]\n",
      "Training epoch#:  109\n",
      "errors (SSE):  [1518.8478876]\n",
      "Training epoch#:  110\n",
      "errors (SSE):  [1517.34718217]\n",
      "Training epoch#:  111\n",
      "errors (SSE):  [1515.88867858]\n",
      "Training epoch#:  112\n",
      "errors (SSE):  [1514.47215347]\n",
      "Training epoch#:  113\n",
      "errors (SSE):  [1513.09720089]\n",
      "Training epoch#:  114\n",
      "errors (SSE):  [1511.76325349]\n",
      "Training epoch#:  115\n",
      "errors (SSE):  [1510.46960312]\n",
      "Training epoch#:  116\n",
      "errors (SSE):  [1509.21542073]\n",
      "Training epoch#:  117\n",
      "errors (SSE):  [1507.99977532]\n",
      "Training epoch#:  118\n",
      "errors (SSE):  [1506.82165175]\n",
      "Training epoch#:  119\n",
      "errors (SSE):  [1505.67996733]\n",
      "Training epoch#:  120\n",
      "errors (SSE):  [1504.57358714]\n",
      "Training epoch#:  121\n",
      "errors (SSE):  [1503.50133803]\n",
      "Training epoch#:  122\n",
      "errors (SSE):  [1502.4620213]\n",
      "Training epoch#:  123\n",
      "errors (SSE):  [1501.454424]\n",
      "Training epoch#:  124\n",
      "errors (SSE):  [1500.4773291]\n",
      "Training epoch#:  125\n",
      "errors (SSE):  [1499.52952427]\n",
      "Training epoch#:  126\n",
      "errors (SSE):  [1498.60980964]\n",
      "Training epoch#:  127\n",
      "errors (SSE):  [1497.71700438]\n",
      "Training epoch#:  128\n",
      "errors (SSE):  [1496.84995232]\n",
      "Training epoch#:  129\n",
      "errors (SSE):  [1496.00752657]\n",
      "Training epoch#:  130\n",
      "errors (SSE):  [1495.18863337]\n",
      "Training epoch#:  131\n",
      "errors (SSE):  [1494.39221507]\n",
      "Training epoch#:  132\n",
      "errors (SSE):  [1493.61725247]\n",
      "Training epoch#:  133\n",
      "errors (SSE):  [1492.86276651]\n",
      "Training epoch#:  134\n",
      "errors (SSE):  [1492.1278194]\n",
      "Training epoch#:  135\n",
      "errors (SSE):  [1491.4115153]\n",
      "Training epoch#:  136\n",
      "errors (SSE):  [1490.7130005]\n",
      "Training epoch#:  137\n",
      "errors (SSE):  [1490.0314633]\n",
      "Training epoch#:  138\n",
      "errors (SSE):  [1489.36613355]\n",
      "Training epoch#:  139\n",
      "errors (SSE):  [1488.71628197]\n",
      "Training epoch#:  140\n",
      "errors (SSE):  [1488.08121916]\n",
      "Training epoch#:  141\n",
      "errors (SSE):  [1487.4602945]\n",
      "Training epoch#:  142\n",
      "errors (SSE):  [1486.85289493]\n",
      "Training epoch#:  143\n",
      "errors (SSE):  [1486.25844353]\n",
      "Training epoch#:  144\n",
      "errors (SSE):  [1485.67639812]\n",
      "Training epoch#:  145\n",
      "errors (SSE):  [1485.10624972]\n",
      "Training epoch#:  146\n",
      "errors (SSE):  [1484.54752102]\n",
      "Training epoch#:  147\n",
      "errors (SSE):  [1483.9997648]\n",
      "Training epoch#:  148\n",
      "errors (SSE):  [1483.46256241]\n",
      "Training epoch#:  149\n",
      "errors (SSE):  [1482.93552217]\n",
      "Training epoch#:  150\n",
      "errors (SSE):  [1482.41827785]\n",
      "Training epoch#:  151\n",
      "errors (SSE):  [1481.91048721]\n",
      "Training epoch#:  152\n",
      "errors (SSE):  [1481.41183047]\n",
      "Training epoch#:  153\n",
      "errors (SSE):  [1480.92200895]\n",
      "Training epoch#:  154\n",
      "errors (SSE):  [1480.44074367]\n",
      "Training epoch#:  155\n",
      "errors (SSE):  [1479.96777404]\n",
      "Training epoch#:  156\n",
      "errors (SSE):  [1479.50285663]\n",
      "Training epoch#:  157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors (SSE):  [1479.0457639]\n",
      "Training epoch#:  158\n",
      "errors (SSE):  [1478.5962831]\n",
      "Training epoch#:  159\n",
      "errors (SSE):  [1478.1542152]\n",
      "Training epoch#:  160\n",
      "errors (SSE):  [1477.7193738]\n",
      "Training epoch#:  161\n",
      "errors (SSE):  [1477.29158417]\n",
      "Training epoch#:  162\n",
      "errors (SSE):  [1476.87068236]\n",
      "Training epoch#:  163\n",
      "errors (SSE):  [1476.45651433]\n",
      "Training epoch#:  164\n",
      "errors (SSE):  [1476.04893508]\n",
      "Training epoch#:  165\n",
      "errors (SSE):  [1475.64780796]\n",
      "Training epoch#:  166\n",
      "errors (SSE):  [1475.25300392]\n",
      "Training epoch#:  167\n",
      "errors (SSE):  [1474.86440084]\n",
      "Training epoch#:  168\n",
      "errors (SSE):  [1474.48188293]\n",
      "Training epoch#:  169\n",
      "errors (SSE):  [1474.10534014]\n",
      "Training epoch#:  170\n",
      "errors (SSE):  [1473.73466763]\n",
      "Training epoch#:  171\n",
      "errors (SSE):  [1473.36976528]\n",
      "Training epoch#:  172\n",
      "errors (SSE):  [1473.01053722]\n",
      "Training epoch#:  173\n",
      "errors (SSE):  [1472.65689141]\n",
      "Training epoch#:  174\n",
      "errors (SSE):  [1472.30873927]\n",
      "Training epoch#:  175\n",
      "errors (SSE):  [1471.96599529]\n",
      "Training epoch#:  176\n",
      "errors (SSE):  [1471.62857672]\n",
      "Training epoch#:  177\n",
      "errors (SSE):  [1471.29640331]\n",
      "Training epoch#:  178\n",
      "errors (SSE):  [1470.96939695]\n",
      "Training epoch#:  179\n",
      "errors (SSE):  [1470.6474815]\n",
      "Training epoch#:  180\n",
      "errors (SSE):  [1470.33058254]\n",
      "Training epoch#:  181\n",
      "errors (SSE):  [1470.01862718]\n",
      "Training epoch#:  182\n",
      "errors (SSE):  [1469.71154383]\n",
      "Training epoch#:  183\n",
      "errors (SSE):  [1469.40926211]\n",
      "Training epoch#:  184\n",
      "errors (SSE):  [1469.11171265]\n",
      "Training epoch#:  185\n",
      "errors (SSE):  [1468.81882699]\n",
      "Training epoch#:  186\n",
      "errors (SSE):  [1468.53053745]\n",
      "Training epoch#:  187\n",
      "errors (SSE):  [1468.24677705]\n",
      "Training epoch#:  188\n",
      "errors (SSE):  [1467.96747941]\n",
      "Training epoch#:  189\n",
      "errors (SSE):  [1467.69257868]\n",
      "Training epoch#:  190\n",
      "errors (SSE):  [1467.42200947]\n",
      "Training epoch#:  191\n",
      "errors (SSE):  [1467.15570681]\n",
      "Training epoch#:  192\n",
      "errors (SSE):  [1466.8936061]\n",
      "Training epoch#:  193\n",
      "errors (SSE):  [1466.63564309]\n",
      "Training epoch#:  194\n",
      "errors (SSE):  [1466.38175381]\n",
      "Training epoch#:  195\n",
      "errors (SSE):  [1466.13187461]\n",
      "Training epoch#:  196\n",
      "errors (SSE):  [1465.8859421]\n",
      "Training epoch#:  197\n",
      "errors (SSE):  [1465.64389315]\n",
      "Training epoch#:  198\n",
      "errors (SSE):  [1465.40566492]\n",
      "Training epoch#:  199\n",
      "errors (SSE):  [1465.1711948]\n",
      "Training epoch#:  200\n",
      "errors (SSE):  [1464.94042049]\n",
      "Training epoch#:  201\n",
      "errors (SSE):  [1464.71327994]\n",
      "Training epoch#:  202\n",
      "errors (SSE):  [1464.48971139]\n",
      "Training epoch#:  203\n",
      "errors (SSE):  [1464.26965342]\n",
      "Training epoch#:  204\n",
      "errors (SSE):  [1464.05304489]\n",
      "Training epoch#:  205\n",
      "errors (SSE):  [1463.83982503]\n",
      "Training epoch#:  206\n",
      "errors (SSE):  [1463.62993342]\n",
      "Training epoch#:  207\n",
      "errors (SSE):  [1463.42331003]\n",
      "Training epoch#:  208\n",
      "errors (SSE):  [1463.21989521]\n",
      "Training epoch#:  209\n",
      "errors (SSE):  [1463.01962976]\n",
      "Training epoch#:  210\n",
      "errors (SSE):  [1462.82245493]\n",
      "Training epoch#:  211\n",
      "errors (SSE):  [1462.62831244]\n",
      "Training epoch#:  212\n",
      "errors (SSE):  [1462.4371445]\n",
      "Training epoch#:  213\n",
      "errors (SSE):  [1462.24889384]\n",
      "Training epoch#:  214\n",
      "errors (SSE):  [1462.06350375]\n",
      "Training epoch#:  215\n",
      "errors (SSE):  [1461.88091806]\n",
      "Training epoch#:  216\n",
      "errors (SSE):  [1461.70108122]\n",
      "Training epoch#:  217\n",
      "errors (SSE):  [1461.52393825]\n",
      "Training epoch#:  218\n",
      "errors (SSE):  [1461.34943485]\n",
      "Training epoch#:  219\n",
      "errors (SSE):  [1461.17751732]\n",
      "Training epoch#:  220\n",
      "errors (SSE):  [1461.00813266]\n",
      "Training epoch#:  221\n",
      "errors (SSE):  [1460.84122855]\n",
      "Training epoch#:  222\n",
      "errors (SSE):  [1460.67675336]\n",
      "Training epoch#:  223\n",
      "errors (SSE):  [1460.51465621]\n",
      "Training epoch#:  224\n",
      "errors (SSE):  [1460.35488694]\n",
      "Training epoch#:  225\n",
      "errors (SSE):  [1460.19739612]\n",
      "Training epoch#:  226\n",
      "errors (SSE):  [1460.04213511]\n",
      "Training epoch#:  227\n",
      "errors (SSE):  [1459.88905602]\n",
      "Training epoch#:  228\n",
      "errors (SSE):  [1459.73811178]\n",
      "Training epoch#:  229\n",
      "errors (SSE):  [1459.58925607]\n",
      "Training epoch#:  230\n",
      "errors (SSE):  [1459.4424434]\n",
      "Training epoch#:  231\n",
      "errors (SSE):  [1459.29762909]\n",
      "Training epoch#:  232\n",
      "errors (SSE):  [1459.15476926]\n",
      "Training epoch#:  233\n",
      "errors (SSE):  [1459.01382085]\n",
      "Training epoch#:  234\n",
      "errors (SSE):  [1458.87474166]\n",
      "Training epoch#:  235\n",
      "errors (SSE):  [1458.73749027]\n",
      "Training epoch#:  236\n",
      "errors (SSE):  [1458.60202613]\n",
      "Training epoch#:  237\n",
      "errors (SSE):  [1458.46830951]\n",
      "Training epoch#:  238\n",
      "errors (SSE):  [1458.33630151]\n",
      "Training epoch#:  239\n",
      "errors (SSE):  [1458.20596405]\n",
      "Training epoch#:  240\n",
      "errors (SSE):  [1458.07725992]\n",
      "Training epoch#:  241\n",
      "errors (SSE):  [1457.9501527]\n",
      "Training epoch#:  242\n",
      "errors (SSE):  [1457.82460683]\n",
      "Training epoch#:  243\n",
      "errors (SSE):  [1457.70058754]\n",
      "Training epoch#:  244\n",
      "errors (SSE):  [1457.57806091]\n",
      "Training epoch#:  245\n",
      "errors (SSE):  [1457.45699382]\n",
      "Training epoch#:  246\n",
      "errors (SSE):  [1457.33735397]\n",
      "Training epoch#:  247\n",
      "errors (SSE):  [1457.21910984]\n",
      "Training epoch#:  248\n",
      "errors (SSE):  [1457.10223074]\n",
      "Training epoch#:  249\n",
      "errors (SSE):  [1456.98668673]\n",
      "[[7, 7], [5, 5], [6, 6], [6, 6], [7, 6], [6, 6], [6, 6], [7, 6], [5, 6], [6, 6], [7, 6], [6, 6], [7, 6], [6, 6], [6, 6], [7, 7], [6, 5], [5, 3], [7, 8], [5, 6], [6, 5], [6, 7], [6, 6], [6, 6], [5, 5], [7, 7], [6, 6], [6, 6], [6, 5], [5, 5], [7, 8], [5, 5], [5, 5], [6, 6], [6, 5], [5, 6], [5, 5], [7, 6], [5, 7], [5, 6], [6, 6], [5, 6], [5, 6], [7, 7], [6, 6], [6, 7], [6, 6], [6, 7], [6, 7], [7, 6], [6, 6], [7, 6], [7, 6], [7, 7], [7, 7], [6, 5], [6, 6], [6, 6], [7, 7], [5, 5], [6, 7], [6, 7], [5, 6], [7, 5], [6, 6], [6, 6], [6, 6], [6, 7], [5, 6], [7, 6], [5, 6], [7, 7], [6, 5], [5, 5], [5, 5], [6, 6], [6, 5], [6, 7], [5, 6], [6, 7], [5, 5], [6, 6], [6, 5], [6, 7], [6, 6], [6, 8], [6, 6], [6, 6], [5, 5], [7, 8], [6, 6], [6, 6], [7, 6], [6, 5], [6, 6], [6, 7], [6, 3], [6, 7], [6, 5], [7, 6], [5, 6], [5, 5], [5, 5], [5, 6], [7, 7], [8, 7], [5, 5], [6, 7], [6, 5], [6, 6], [6, 6], [6, 6], [5, 5], [6, 6], [5, 6], [5, 7], [6, 6], [5, 6], [5, 5], [6, 6], [6, 6], [6, 7], [6, 6], [6, 6], [6, 6], [6, 6], [6, 7], [6, 7], [6, 5], [6, 6], [7, 6], [5, 5], [6, 7], [4, 4], [6, 6], [5, 5], [7, 6], [7, 8], [6, 7], [6, 6], [6, 6], [7, 6], [6, 6], [6, 7], [5, 5], [6, 6], [5, 6], [6, 6], [6, 6], [5, 5], [5, 6], [6, 6], [5, 5], [6, 6], [5, 5], [6, 6], [7, 6], [6, 8], [6, 7], [5, 6], [6, 5], [6, 6], [6, 5], [6, 7], [7, 7], [7, 7], [6, 6], [5, 5], [6, 5], [6, 5], [7, 6], [6, 5], [5, 6], [5, 5], [6, 7], [6, 6], [5, 5], [7, 6], [5, 5], [7, 6], [7, 9], [6, 6], [5, 6], [6, 7], [6, 7], [6, 6], [5, 5], [7, 7], [6, 7], [5, 5], [6, 8], [5, 5], [6, 6], [5, 6], [5, 6], [5, 4], [7, 8], [6, 6], [6, 6], [5, 6], [6, 6], [6, 6], [6, 6], [5, 6], [7, 7], [5, 5], [6, 6], [5, 6], [5, 5], [6, 5], [6, 6], [7, 7], [6, 7], [6, 6], [6, 5], [5, 7], [5, 5], [7, 7], [6, 3], [6, 6], [6, 6], [7, 7], [6, 6], [6, 5], [5, 5], [7, 7], [6, 5], [6, 8], [5, 6], [6, 6], [5, 6], [6, 7], [5, 5], [5, 5], [5, 5], [5, 5], [6, 6], [6, 6], [6, 6], [6, 6], [6, 6], [7, 6], [6, 6], [7, 8], [6, 6], [6, 5], [5, 5], [5, 5], [6, 6], [5, 5], [6, 6], [6, 5], [7, 7], [6, 6], [6, 7], [5, 6], [6, 5], [6, 7], [6, 7], [5, 6], [5, 5], [6, 8], [5, 5], [7, 6], [7, 7], [6, 6], [6, 8], [6, 5], [6, 6], [6, 7], [6, 6], [5, 5], [6, 7], [6, 6], [6, 6], [6, 6], [6, 7], [7, 6], [6, 6], [5, 6], [6, 6], [5, 5], [5, 6], [7, 6], [6, 8], [6, 6], [5, 7], [7, 4], [7, 7], [6, 7], [5, 5], [5, 6], [6, 6], [6, 4], [6, 5], [6, 7], [6, 6], [5, 6], [6, 7], [6, 6], [6, 6], [5, 6], [6, 6], [7, 6], [6, 6], [6, 7], [7, 5], [6, 7], [7, 5], [6, 6], [5, 6], [6, 6], [6, 6], [7, 7], [6, 6], [6, 6], [5, 4], [7, 6], [5, 5], [6, 6], [6, 6], [6, 6], [5, 5], [6, 7], [5, 5], [6, 7], [6, 6], [5, 6], [6, 7], [5, 5], [6, 7], [5, 5], [5, 7], [6, 8], [6, 7], [5, 6], [6, 4], [6, 5], [6, 5], [7, 7], [5, 6], [7, 6], [7, 7], [5, 6], [5, 5], [5, 6], [5, 5], [7, 6], [6, 8], [7, 7], [6, 5], [6, 6], [5, 6], [6, 7], [6, 5], [5, 5], [5, 5], [5, 4], [6, 6], [6, 5], [5, 7], [6, 8], [6, 7], [6, 7], [6, 6], [5, 5], [5, 5], [5, 4], [6, 5], [5, 6], [6, 6], [5, 5], [6, 6], [5, 6], [5, 5], [6, 6], [8, 7], [6, 6], [7, 6], [6, 5], [7, 6], [5, 5], [6, 6], [6, 6], [6, 6], [6, 5], [5, 6], [5, 5], [5, 5], [6, 7], [6, 6], [7, 7], [5, 5], [7, 7], [6, 4], [7, 7], [5, 7], [6, 6], [6, 6], [6, 6], [6, 7], [5, 5], [6, 6], [6, 6], [5, 5], [5, 6], [6, 4], [7, 6], [5, 5], [6, 6], [5, 7], [6, 6], [6, 5], [5, 4], [6, 6], [6, 6], [6, 7], [5, 5], [7, 6], [6, 5], [6, 6], [7, 6], [5, 6], [6, 5], [6, 5], [6, 6], [6, 4], [5, 6], [5, 5], [7, 6], [6, 7], [6, 5], [5, 6], [6, 5], [6, 7], [6, 7], [5, 5], [5, 5], [6, 6], [7, 6], [7, 7], [6, 6], [5, 5], [7, 7], [7, 7], [7, 7], [6, 7], [6, 6], [6, 6], [7, 7], [6, 6], [6, 6], [6, 5], [6, 7], [7, 7], [6, 6], [7, 6], [6, 6], [5, 5], [5, 6], [5, 4], [6, 5], [5, 7], [6, 6], [7, 6], [6, 6], [6, 6], [5, 5], [6, 5], [6, 5], [6, 6], [6, 7], [5, 5], [5, 5], [5, 5], [7, 8], [6, 6], [6, 6], [5, 5], [6, 5], [5, 5], [5, 5], [6, 7], [5, 6], [6, 7], [6, 8], [6, 6], [6, 7], [6, 5], [5, 4], [5, 7], [5, 3], [6, 7], [6, 6], [5, 5], [6, 6], [6, 6], [6, 6], [6, 6], [5, 6], [7, 6], [6, 6], [5, 5], [6, 7], [6, 6], [5, 6], [6, 7], [7, 8], [6, 6], [6, 6], [7, 6], [6, 7], [5, 5], [6, 6], [6, 6], [6, 7], [6, 6], [5, 7], [5, 5], [5, 5], [5, 5], [7, 8], [5, 5], [6, 6], [6, 6], [6, 5], [5, 5], [5, 6], [5, 5], [7, 7], [6, 6], [5, 5], [6, 7], [5, 5], [6, 6], [5, 6], [6, 5], [5, 5], [6, 6], [6, 8], [6, 6], [5, 6], [6, 5], [6, 5], [6, 6], [6, 6], [7, 6], [6, 7], [6, 6], [6, 6], [5, 5], [7, 6], [6, 3], [7, 6], [6, 5], [6, 6], [5, 5], [6, 6], [6, 5], [7, 6], [6, 5], [6, 6], [6, 6], [5, 5], [5, 5], [6, 6], [6, 6], [6, 7], [6, 7], [6, 7], [6, 6], [6, 4], [6, 6], [5, 4], [6, 6], [5, 6], [6, 6], [6, 7], [6, 6], [6, 6], [5, 6], [5, 5], [7, 6], [6, 7], [6, 6], [5, 5], [5, 4], [6, 6], [5, 6], [6, 6], [7, 5], [6, 7], [6, 6], [6, 5], [5, 6], [6, 6], [7, 7], [6, 6], [7, 8], [5, 5], [5, 5], [6, 6], [6, 7], [6, 5], [6, 8], [6, 6], [6, 7], [6, 6], [6, 7], [6, 5], [6, 5], [5, 6], [6, 5], [6, 5], [5, 6], [6, 6], [6, 5], [5, 6], [7, 7], [6, 6], [6, 8], [6, 5], [5, 6], [6, 6], [5, 6], [5, 7], [6, 6], [7, 6], [6, 6], [5, 6], [5, 6], [6, 5], [5, 6], [5, 5], [5, 5], [6, 6], [5, 6], [5, 5], [6, 6], [6, 8], [7, 8], [6, 5], [5, 6], [6, 6], [6, 5], [6, 6], [7, 7], [5, 5], [6, 6], [5, 5], [6, 5], [5, 5], [5, 5], [6, 7], [7, 6], [6, 5], [6, 8], [6, 5], [5, 5], [6, 5], [5, 6], [5, 6], [6, 6], [5, 5], [5, 3], [6, 7], [7, 7], [7, 7], [6, 6], [6, 6], [5, 6], [5, 6], [6, 6], [6, 6], [5, 6], [7, 5], [6, 6], [6, 7], [6, 6], [6, 6], [6, 6], [6, 5], [5, 6], [5, 5], [6, 6], [5, 5], [5, 6], [5, 6], [5, 5], [7, 8], [5, 6], [6, 4], [6, 5], [6, 6], [6, 5], [6, 6], [6, 7], [6, 7], [5, 5], [6, 6], [6, 6], [5, 6], [5, 4], [6, 8], [6, 6], [5, 6], [5, 5], [5, 6], [5, 5], [6, 7], [6, 7], [6, 5], [5, 5], [6, 7], [5, 5], [6, 7], [5, 5], [5, 5], [5, 6], [6, 7], [6, 5], [7, 8], [5, 6], [5, 5], [6, 6], [5, 5], [6, 6], [6, 6], [6, 7], [7, 7], [6, 6], [5, 5], [6, 6], [5, 7], [6, 5], [5, 5], [6, 7], [7, 7], [7, 6], [6, 7], [6, 6], [6, 6], [5, 5], [6, 7], [6, 6], [7, 5], [5, 5], [7, 6], [6, 6], [6, 6], [6, 6], [7, 5], [5, 5], [5, 5], [5, 5], [6, 6], [6, 7], [6, 6], [6, 6], [6, 6], [7, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 6], [5, 5], [5, 6], [6, 5], [5, 6], [5, 5], [5, 5], [5, 4], [5, 7], [6, 6], [5, 5], [6, 6], [5, 6], [6, 7], [6, 5], [5, 7], [6, 6], [6, 9], [6, 4], [6, 5], [6, 5], [6, 6], [6, 6], [6, 7], [5, 7], [5, 5], [6, 5], [7, 5], [7, 7], [6, 5], [5, 5], [6, 5], [6, 6], [7, 7], [6, 6], [5, 5], [6, 6], [6, 6], [7, 8], [6, 8], [6, 6], [5, 5], [5, 5], [7, 8], [6, 6], [6, 7], [6, 5], [6, 6], [5, 6], [6, 6], [6, 6], [6, 6], [5, 6], [6, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 6], [6, 6], [5, 5], [6, 5], [5, 5], [7, 6], [4, 4], [6, 6], [6, 5], [6, 6], [6, 6], [6, 6], [6, 7], [5, 6], [6, 6], [6, 6], [6, 6], [6, 7], [6, 5], [6, 6], [6, 7], [6, 6], [6, 5], [6, 5], [6, 6], [5, 5], [5, 7], [5, 6], [6, 6], [6, 6], [5, 6], [6, 6], [5, 5], [6, 7], [6, 6], [5, 5], [5, 6], [6, 5], [5, 6], [6, 7], [7, 8], [6, 5], [5, 5], [7, 6], [6, 4], [6, 6], [6, 5], [6, 6], [5, 6], [5, 5], [6, 8], [5, 5], [6, 5], [6, 7], [6, 6], [6, 6], [6, 6], [6, 5], [5, 6], [6, 4], [6, 5], [7, 6], [5, 6], [6, 5], [5, 5], [6, 7], [6, 6], [6, 6], [7, 8], [6, 5], [5, 6], [6, 7], [6, 6], [6, 5], [5, 6], [5, 5], [6, 5], [6, 6], [6, 5], [6, 6], [6, 5], [5, 5], [6, 6], [6, 7], [6, 5], [6, 6], [6, 5], [6, 6], [5, 7], [6, 6], [7, 7], [5, 6], [6, 6], [6, 6], [5, 5], [7, 7], [6, 6], [7, 7], [5, 7], [6, 6], [5, 5], [5, 6], [6, 5], [5, 6], [4, 5], [6, 6], [6, 6], [7, 7], [6, 5], [6, 5], [5, 6], [6, 7], [5, 5], [6, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 7], [7, 7], [6, 5], [6, 7], [6, 7], [6, 5], [6, 6], [6, 6], [5, 5], [5, 5], [6, 5], [5, 5], [5, 5], [6, 6], [6, 5], [7, 7], [5, 5], [6, 6], [5, 4], [6, 5], [6, 6], [6, 6], [6, 6], [6, 5], [5, 5], [5, 5], [6, 6], [7, 4], [5, 6], [7, 6], [6, 6], [6, 7], [6, 6], [6, 6], [5, 5], [7, 7], [6, 7], [6, 5], [5, 6], [6, 5], [5, 5], [6, 4], [6, 7], [6, 7], [6, 6], [5, 5], [7, 7], [7, 6], [5, 5], [7, 5], [5, 7], [6, 6], [5, 6], [6, 7], [5, 5], [7, 7], [5, 6], [6, 6], [7, 6], [5, 6], [5, 5], [6, 6], [5, 5], [5, 5], [5, 5], [5, 5], [6, 6], [6, 7], [6, 5], [6, 7], [6, 6], [6, 6], [6, 7], [6, 7], [7, 6], [5, 5], [5, 7], [5, 5], [6, 6], [7, 5], [6, 6], [6, 6], [7, 7], [6, 6], [7, 7], [6, 6], [5, 6], [5, 6], [6, 6], [6, 6], [5, 6], [6, 6], [6, 7], [5, 5], [6, 6], [6, 6], [6, 6], [6, 7], [5, 6], [6, 6], [6, 6], [7, 8], [5, 5], [6, 6], [5, 5], [5, 5], [6, 7], [6, 6], [5, 5], [5, 4], [6, 7], [5, 5], [6, 7], [5, 5], [6, 5], [5, 6], [5, 6], [5, 5], [6, 5], [5, 5], [7, 6], [6, 6], [6, 6], [5, 5], [6, 6], [6, 7], [5, 6], [6, 5], [6, 6], [5, 7], [6, 7], [6, 4], [6, 6], [6, 6], [5, 5], [6, 6], [7, 7], [5, 6], [6, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 6], [7, 7], [6, 5], [5, 5], [7, 6], [6, 4], [5, 5], [5, 5], [6, 6], [5, 6], [6, 6], [7, 4], [7, 6], [5, 5], [5, 6], [6, 6], [6, 8], [7, 7], [6, 6], [6, 6], [5, 5], [5, 5], [6, 6], [5, 7], [5, 5], [7, 8], [6, 5], [7, 6], [5, 4], [5, 7], [5, 7], [7, 6], [6, 6], [7, 7], [7, 7], [6, 6], [5, 6], [6, 7], [6, 7], [5, 6], [5, 6], [5, 5], [5, 6], [5, 5], [6, 7], [7, 6], [6, 6], [6, 6], [6, 5], [6, 6], [5, 5], [5, 6], [5, 5], [7, 7], [6, 6], [5, 5], [6, 6], [6, 6], [7, 7], [5, 6], [6, 7], [6, 6], [6, 6], [6, 7], [6, 5], [7, 7], [6, 7], [6, 6], [6, 6], [5, 5], [6, 5], [5, 5], [5, 5], [6, 6], [7, 7], [4, 4], [6, 6], [5, 6], [6, 6], [5, 5], [6, 6], [5, 6], [6, 8], [5, 6], [6, 6], [5, 4], [6, 5], [6, 6], [6, 6], [7, 6], [6, 5], [5, 5], [6, 6], [5, 6], [7, 7], [7, 5], [6, 5], [6, 7], [7, 7], [6, 6], [5, 6], [6, 8], [7, 8], [6, 7], [6, 6], [6, 6], [5, 5], [6, 6], [6, 5], [6, 6], [4, 5], [6, 6], [6, 6], [7, 7], [5, 5], [5, 6], [5, 5], [5, 6], [6, 6], [6, 7], [5, 6], [6, 6], [6, 5], [7, 6], [6, 6], [6, 5], [6, 6], [6, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 7], [5, 7], [6, 6], [6, 7], [5, 5], [6, 6], [5, 5], [6, 4], [6, 8], [6, 5], [5, 4], [5, 5], [6, 7], [6, 8], [6, 7], [6, 7], [6, 6], [6, 6], [5, 6], [5, 5], [7, 6], [6, 6], [6, 6], [7, 7], [6, 5], [6, 6], [6, 6], [5, 6], [5, 6], [5, 5], [6, 7], [5, 7], [6, 6], [5, 5], [5, 5], [6, 6], [6, 5], [6, 6], [6, 6], [6, 7], [5, 6], [5, 5], [5, 5], [6, 7], [6, 7], [6, 3], [6, 6], [6, 5], [6, 7], [6, 6], [7, 6], [7, 6], [5, 5], [6, 6], [6, 6], [5, 5], [6, 6], [6, 7], [6, 5], [7, 6], [6, 5], [6, 6], [6, 6], [6, 4], [6, 6], [5, 5], [7, 7], [5, 6], [5, 6], [6, 6], [6, 5], [6, 6], [5, 5], [6, 8], [5, 6], [6, 6], [6, 6], [6, 6], [5, 7], [6, 7], [5, 4], [6, 6], [5, 5], [6, 6], [5, 5], [5, 7], [5, 6], [6, 6], [5, 6], [6, 6], [6, 7], [7, 6], [6, 6], [5, 5], [6, 6], [5, 5], [6, 7], [5, 4], [5, 5], [7, 8], [5, 5], [7, 6], [6, 6], [5, 5], [7, 7], [6, 6], [6, 6], [7, 6], [6, 5], [6, 7], [6, 5], [5, 5], [5, 6], [6, 6], [6, 6], [6, 7], [6, 7], [7, 6], [5, 6], [5, 5], [6, 7], [7, 6], [6, 6], [6, 6], [6, 5], [5, 5], [6, 6], [6, 6], [5, 5], [5, 5], [7, 7], [6, 6], [6, 6], [6, 7], [6, 6], [6, 7], [6, 7], [6, 5], [7, 6], [5, 5], [7, 7], [5, 6], [5, 6], [6, 6], [5, 5], [7, 6], [6, 7], [5, 5], [5, 5], [6, 6], [6, 6], [6, 5], [7, 7], [6, 6], [5, 5], [5, 6], [5, 6], [6, 6], [7, 7], [5, 5], [6, 6], [5, 6], [5, 6], [5, 6], [6, 5], [6, 6], [6, 6], [5, 6], [6, 6], [6, 6], [5, 6], [6, 6], [6, 8], [6, 6], [5, 5], [6, 6], [7, 7], [6, 6], [6, 4], [6, 6], [7, 6], [5, 6], [7, 6], [6, 7], [6, 6], [7, 5], [7, 7], [6, 5], [5, 5], [5, 6], [7, 6], [6, 7], [5, 6], [5, 5], [6, 6], [7, 6], [6, 6], [6, 7], [5, 5], [6, 6], [6, 6], [6, 6], [6, 6], [6, 6], [5, 4], [6, 6], [7, 7], [6, 4], [6, 4], [6, 5], [5, 5], [5, 5], [6, 7], [6, 4], [6, 5], [6, 6], [6, 6], [7, 8], [7, 7], [6, 6], [6, 6], [5, 6], [5, 5], [6, 6], [5, 5], [5, 5], [5, 4], [5, 6], [7, 6], [7, 6], [6, 5], [6, 6], [5, 5], [6, 6], [5, 7], [5, 6], [6, 5], [5, 5], [7, 6], [6, 6], [5, 5], [5, 5], [5, 6], [6, 6], [6, 6], [7, 6], [6, 6], [5, 6], [6, 6], [7, 6], [6, 6], [5, 6], [6, 6], [5, 5], [6, 7], [6, 6], [6, 6], [6, 7], [5, 6], [6, 5], [6, 5], [7, 7], [5, 5], [6, 6], [6, 7], [6, 6], [6, 6], [6, 5], [6, 8], [6, 5], [6, 7], [6, 6], [5, 6], [6, 6], [5, 5], [6, 5], [6, 6], [5, 6], [6, 5], [7, 7], [5, 6], [6, 5], [5, 7], [5, 5], [5, 5], [6, 5], [7, 7], [5, 6], [6, 7], [6, 6], [6, 7], [5, 7], [5, 5], [6, 7], [6, 7], [5, 5], [6, 5], [6, 6], [7, 5], [6, 6], [5, 5], [6, 6], [6, 7], [7, 7], [6, 5], [6, 6], [6, 6], [6, 6], [6, 5], [5, 5], [5, 5], [5, 5], [6, 6], [6, 6], [6, 8], [6, 7], [6, 6], [6, 6], [5, 5], [5, 5], [6, 6], [6, 5], [5, 6], [6, 7], [5, 6], [5, 5], [6, 5], [5, 7], [5, 6], [5, 5], [6, 6], [5, 6], [5, 7], [5, 5], [6, 5], [6, 6], [6, 6], [6, 6], [5, 6], [5, 5], [6, 8], [6, 6], [5, 5], [5, 6], [7, 6], [6, 8], [6, 6], [5, 6], [6, 6], [6, 6], [6, 5], [7, 8], [6, 7], [5, 5], [6, 6], [5, 5], [6, 6], [7, 7], [5, 5], [6, 6], [7, 6], [6, 5], [6, 6], [5, 5], [5, 4], [6, 7], [6, 6], [6, 6], [5, 5], [5, 6], [6, 7], [6, 5], [6, 6], [7, 6], [7, 7], [5, 5], [5, 5], [5, 5], [6, 8], [5, 5], [5, 8], [5, 5], [6, 7], [7, 6], [5, 4], [5, 4], [6, 6], [6, 5], [6, 6], [7, 6], [6, 6], [6, 5], [6, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 6], [7, 6], [6, 8], [6, 6], [6, 7], [6, 4], [6, 5], [7, 7], [5, 5], [5, 5], [6, 7], [5, 6], [6, 6], [6, 5], [5, 4], [5, 6], [5, 6], [6, 6], [5, 6], [6, 8], [6, 7], [6, 6], [6, 7], [5, 6], [5, 6], [6, 7], [6, 6], [7, 7], [7, 7], [6, 6], [5, 5], [6, 6], [5, 6], [6, 6], [7, 7], [6, 5], [6, 8], [6, 6], [6, 6], [6, 5], [6, 6], [6, 7], [5, 6], [6, 6], [6, 6], [6, 5], [6, 5], [5, 5], [6, 6], [5, 4], [6, 6], [8, 8], [6, 6], [6, 5], [7, 7], [6, 6], [5, 6], [6, 7], [5, 5], [6, 6], [6, 7], [6, 6], [6, 6], [7, 6], [6, 6], [6, 7], [5, 5], [7, 7], [5, 4], [5, 5], [6, 5], [6, 7], [6, 6], [6, 5], [6, 5], [6, 6], [5, 6], [6, 6], [6, 7], [5, 6], [6, 6], [5, 6], [6, 7], [6, 6], [6, 7], [6, 7], [5, 5], [5, 5], [6, 6], [7, 7], [5, 5], [5, 6], [5, 6], [5, 5], [5, 5], [5, 5], [7, 8], [6, 6], [6, 6], [6, 6], [5, 5], [6, 6], [7, 7], [6, 7], [5, 6], [5, 5], [6, 6], [6, 6], [5, 5], [6, 6], [6, 7], [6, 8], [6, 5], [7, 7], [7, 7], [7, 7], [5, 4], [6, 6], [5, 5], [6, 8], [5, 6], [5, 5], [5, 5], [6, 6], [6, 5], [6, 6], [6, 6], [6, 7], [5, 5], [6, 6], [6, 6], [5, 5], [6, 5], [6, 6], [6, 7], [5, 7], [7, 7], [5, 5], [6, 6], [6, 6], [5, 7], [6, 5], [6, 6], [6, 4], [6, 5], [6, 6], [6, 7], [6, 6], [7, 7], [5, 4], [7, 7], [5, 5], [6, 6], [7, 7], [6, 6], [6, 6], [6, 6], [5, 6], [5, 5], [6, 6], [6, 6], [6, 6], [5, 6], [5, 6], [5, 6], [7, 6], [6, 6], [6, 7], [7, 6], [6, 4], [7, 7], [6, 7], [6, 7], [6, 6], [5, 5], [5, 5], [6, 5], [6, 6], [6, 6], [6, 7], [6, 6], [6, 7], [7, 5], [7, 7], [6, 7], [5, 5], [6, 6], [6, 6], [7, 5], [6, 6], [6, 5], [5, 5], [5, 5], [6, 6], [5, 7], [6, 7], [6, 5], [6, 5], [6, 7], [6, 6], [7, 7], [5, 7], [6, 6], [6, 6], [6, 6], [5, 7], [6, 6], [6, 6], [6, 8], [6, 5], [5, 5], [5, 6], [6, 6], [6, 6], [5, 5], [6, 6], [6, 7], [6, 7], [5, 7], [5, 5], [6, 8], [6, 6], [6, 5], [5, 5], [5, 5], [5, 5], [5, 5], [6, 6], [6, 7], [6, 5], [6, 6], [5, 6], [5, 5], [5, 6], [5, 6], [6, 6], [7, 7], [6, 6], [6, 6], [6, 7], [7, 7], [7, 6], [6, 7], [5, 5], [6, 6], [7, 6], [6, 6], [6, 5], [5, 7], [5, 6], [5, 6], [5, 6], [5, 5], [5, 5], [6, 6], [6, 5], [6, 6], [6, 6], [5, 5], [6, 5], [5, 5], [6, 6], [5, 6], [6, 6], [5, 5], [6, 6], [5, 5], [7, 7], [5, 5], [5, 5], [6, 5], [6, 5], [6, 5], [6, 6], [7, 5], [7, 7], [6, 6], [5, 4], [6, 7], [6, 6], [6, 6], [5, 6], [5, 5], [5, 6], [5, 5], [6, 5], [7, 7], [6, 6], [6, 5], [6, 5], [5, 5], [6, 6], [6, 6], [5, 5], [5, 5], [6, 6], [5, 6], [6, 5], [6, 5], [5, 5], [5, 6], [5, 5], [5, 5], [6, 5], [5, 7], [7, 6], [7, 7], [5, 5], [6, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 5], [5, 6], [5, 7], [5, 5], [5, 5], [6, 6], [7, 7], [5, 6], [6, 6], [6, 6], [6, 5], [7, 6], [5, 6], [6, 6], [6, 6], [6, 6], [6, 7], [6, 6], [7, 5], [6, 5], [6, 6], [7, 7], [7, 8], [5, 7], [6, 6], [6, 8], [6, 6], [6, 7], [6, 5], [6, 5], [6, 6], [6, 6], [6, 6], [7, 7], [6, 7], [6, 5], [5, 6], [5, 5], [6, 6], [7, 5], [7, 6], [5, 6], [5, 5], [6, 5], [5, 6], [6, 6], [6, 6], [6, 4], [6, 6], [5, 5], [6, 6], [5, 5], [5, 5], [5, 5], [5, 6], [5, 5], [6, 7], [7, 7], [6, 5], [6, 6], [6, 6], [6, 6], [5, 5], [6, 6], [5, 6], [5, 5], [6, 6], [6, 7], [7, 7], [6, 5], [5, 5], [6, 6], [6, 6], [5, 5], [6, 6], [6, 7], [6, 6], [7, 6], [6, 4], [6, 6], [7, 6], [6, 6], [6, 6], [6, 7], [5, 5], [5, 7], [5, 5], [6, 7], [6, 6], [7, 6], [6, 6], [7, 7], [6, 6], [6, 5], [6, 6], [4, 6], [5, 7], [5, 5], [6, 5], [6, 6], [5, 5], [5, 5], [5, 5], [6, 6], [6, 6], [5, 6], [6, 6], [5, 4], [5, 5], [6, 6], [6, 6], [6, 6], [6, 7], [5, 4], [5, 4], [6, 5], [6, 6], [6, 6], [6, 8], [6, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 5], [5, 5], [5, 5], [5, 6], [5, 6], [7, 6], [5, 6], [6, 6], [5, 5], [6, 7], [5, 4], [7, 6], [5, 5], [5, 5], [6, 5], [6, 6], [7, 6], [6, 6], [7, 7], [5, 6], [6, 5], [7, 7], [6, 5], [6, 6], [5, 6], [7, 6], [6, 5], [5, 5], [5, 5], [5, 5], [6, 5], [7, 6], [5, 6], [7, 5], [5, 5], [5, 6], [5, 5], [6, 6], [6, 6], [6, 6], [5, 5], [6, 6], [6, 7], [5, 6], [6, 6], [6, 5], [7, 7], [6, 6], [6, 6], [6, 7], [6, 5], [6, 6], [5, 4], [5, 4], [6, 5], [6, 8], [6, 6], [6, 6], [6, 6], [5, 6], [4, 4], [6, 6], [6, 6], [5, 5], [7, 6], [6, 7], [6, 7], [6, 5], [6, 6], [6, 6], [6, 5], [6, 7], [5, 5], [6, 6], [6, 7], [5, 5], [6, 4], [7, 7], [6, 7], [6, 7], [6, 8], [6, 6], [5, 5], [6, 6], [5, 5], [7, 8], [5, 5], [6, 6], [5, 7], [7, 7], [5, 5], [5, 5], [6, 6], [6, 6], [6, 7], [6, 7], [6, 6], [5, 5], [6, 5], [6, 5], [6, 6], [6, 7], [7, 6], [7, 6], [5, 5], [6, 6], [6, 6], [6, 6], [6, 6], [6, 6], [6, 7], [6, 7], [5, 6], [6, 6], [5, 5], [5, 5], [5, 6], [5, 6], [6, 5], [5, 5], [6, 7], [6, 8], [6, 6], [6, 6], [6, 7], [5, 5], [7, 6], [5, 4], [6, 5], [5, 5], [6, 6], [6, 6], [6, 7], [5, 5], [5, 5], [5, 5], [7, 5], [5, 5], [5, 5], [7, 7], [6, 6], [5, 5], [5, 5], [5, 7], [5, 6], [6, 6], [8, 7], [6, 6], [5, 6], [5, 6], [6, 6], [5, 5], [6, 6], [6, 6], [6, 6], [6, 7], [7, 6], [6, 6], [5, 4], [6, 7], [6, 6], [6, 4], [7, 7], [7, 6], [6, 7], [6, 4], [5, 5], [5, 5], [6, 5], [6, 5], [7, 6], [6, 7], [5, 7], [6, 5], [5, 6], [5, 5], [5, 5], [6, 6], [6, 6], [6, 8], [6, 7], [6, 6], [7, 6], [5, 6], [5, 5], [6, 5], [6, 6], [5, 6], [6, 6], [5, 5], [6, 6], [6, 7], [6, 6], [6, 5], [6, 6], [5, 6], [5, 5], [5, 6], [5, 6], [5, 7], [6, 6], [6, 6], [6, 5], [5, 8], [5, 5], [6, 8], [6, 5], [7, 5], [5, 6], [5, 5], [5, 6], [6, 4], [6, 7], [6, 6], [5, 5], [6, 5], [5, 5], [6, 6], [7, 6], [6, 7], [6, 6], [5, 6], [6, 7], [6, 6], [6, 5], [5, 6], [7, 6], [6, 6], [6, 5], [7, 4], [6, 5], [5, 5], [7, 7], [7, 8], [6, 6], [6, 5], [6, 7], [6, 5], [5, 6], [6, 6], [6, 5], [6, 7], [6, 4], [5, 5], [5, 6], [6, 6], [5, 6], [7, 8], [6, 6], [6, 7], [6, 7], [6, 7], [6, 7], [6, 5], [6, 6], [6, 7], [5, 6], [7, 6], [7, 8], [6, 5], [6, 7], [6, 6], [6, 6], [5, 4], [6, 5], [5, 4], [5, 6], [6, 6], [6, 6], [7, 6], [6, 6], [6, 6], [5, 4], [5, 5], [6, 5], [7, 5], [6, 4], [6, 5], [7, 6], [5, 5], [5, 6], [6, 6], [6, 8], [7, 8], [5, 5], [5, 4], [7, 7], [5, 6], [7, 6], [7, 7], [7, 6], [6, 7], [7, 7], [6, 6], [5, 5], [5, 5], [5, 6], [5, 5], [6, 6], [5, 5], [6, 6], [5, 6], [6, 6], [6, 7], [6, 4], [6, 7], [5, 5], [6, 5], [6, 6], [6, 7], [6, 6], [5, 5], [6, 5], [6, 7], [6, 6], [5, 4], [5, 6], [6, 7], [6, 6], [5, 5], [5, 6], [7, 5], [6, 7], [6, 6], [6, 7], [6, 7], [6, 7], [6, 6], [6, 6], [6, 7], [6, 5], [6, 5], [6, 6], [5, 7], [6, 8], [5, 5], [7, 6], [6, 6], [6, 6], [6, 6], [6, 6], [6, 6], [7, 7], [5, 5], [5, 5], [6, 6], [6, 6], [7, 8], [6, 6], [5, 5], [5, 5], [6, 6], [5, 6], [6, 5], [5, 5], [6, 5], [5, 6], [5, 5], [7, 7], [6, 5], [6, 7], [6, 5], [6, 6], [5, 5], [6, 5], [5, 5], [5, 7], [5, 6], [7, 7], [6, 4], [6, 6], [5, 5], [6, 6], [6, 6], [6, 6], [5, 5], [6, 8], [6, 6], [5, 5], [5, 5], [6, 6], [6, 5], [6, 6], [5, 5], [6, 5], [6, 6], [6, 6], [6, 7], [6, 7], [6, 5], [7, 8], [5, 6], [6, 7], [5, 4], [6, 5], [6, 7], [5, 6], [7, 7], [5, 7]]\n"
     ]
    }
   ],
   "source": [
    "hybrid = ANNkNNHybrid(datasets['wine']['inputnodes'],\n",
    "                   datasets['wine']['hiddennodes'],\n",
    "                   datasets['wine']['outputnodes'],\n",
    "                   datasets['wine']['lr'],\n",
    "                   datasets['wine']['bs'],\n",
    "                   datasets['wine']['epochs'],\n",
    "                      datasets['wine']['k'],\n",
    "         weighted=datasets['wine']['weighted'])\n",
    "hybrid.train(datasets['wine']['train'])\n",
    "#Test\n",
    "hybrid.test(datasets['wine']['test'])\n",
    "wine_results.append(accuracy(hybrid.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Breast Cancer Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch#:  0\n",
      "errors (SSE):  [142.76446888]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [139.74440685]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [137.25784766]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [133.70721552]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [128.48472543]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [121.02488405]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [111.15011741]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [99.47335606]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [87.32851316]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [76.07617435]\n",
      "Training epoch#:  10\n",
      "errors (SSE):  [66.49629164]\n",
      "Training epoch#:  11\n",
      "errors (SSE):  [58.72767518]\n",
      "Training epoch#:  12\n",
      "errors (SSE):  [52.54783107]\n",
      "Training epoch#:  13\n",
      "errors (SSE):  [47.63941219]\n",
      "Training epoch#:  14\n",
      "errors (SSE):  [43.71322103]\n",
      "Training epoch#:  15\n",
      "errors (SSE):  [40.53804828]\n",
      "Training epoch#:  16\n",
      "errors (SSE):  [37.9371242]\n",
      "Training epoch#:  17\n",
      "errors (SSE):  [35.77721619]\n",
      "Training epoch#:  18\n",
      "errors (SSE):  [33.95822368]\n",
      "Training epoch#:  19\n",
      "errors (SSE):  [32.40487929]\n",
      "Training epoch#:  20\n",
      "errors (SSE):  [31.0604437]\n",
      "Training epoch#:  21\n",
      "errors (SSE):  [29.88197948]\n",
      "Training epoch#:  22\n",
      "errors (SSE):  [28.83683104]\n",
      "Training epoch#:  23\n",
      "errors (SSE):  [27.90001561]\n",
      "Training epoch#:  24\n",
      "errors (SSE):  [27.05229468]\n",
      "Training epoch#:  25\n",
      "errors (SSE):  [26.27874827]\n",
      "Training epoch#:  26\n",
      "errors (SSE):  [25.56771806]\n",
      "Training epoch#:  27\n",
      "errors (SSE):  [24.91002117]\n",
      "Training epoch#:  28\n",
      "errors (SSE):  [24.29836267]\n",
      "Training epoch#:  29\n",
      "errors (SSE):  [23.72689494]\n",
      "Training epoch#:  30\n",
      "errors (SSE):  [23.19088575]\n",
      "Training epoch#:  31\n",
      "errors (SSE):  [22.68646745]\n",
      "Training epoch#:  32\n",
      "errors (SSE):  [22.21044661]\n",
      "Training epoch#:  33\n",
      "errors (SSE):  [21.76015909]\n",
      "Training epoch#:  34\n",
      "errors (SSE):  [21.33335927]\n",
      "Training epoch#:  35\n",
      "errors (SSE):  [20.92813495]\n",
      "Training epoch#:  36\n",
      "errors (SSE):  [20.54284177]\n",
      "Training epoch#:  37\n",
      "errors (SSE):  [20.17605233]\n",
      "Training epoch#:  38\n",
      "errors (SSE):  [19.82651661]\n",
      "Training epoch#:  39\n",
      "errors (SSE):  [19.49313089]\n",
      "Training epoch#:  40\n",
      "errors (SSE):  [19.17491318]\n",
      "Training epoch#:  41\n",
      "errors (SSE):  [18.87098377]\n",
      "Training epoch#:  42\n",
      "errors (SSE):  [18.58054958]\n",
      "Training epoch#:  43\n",
      "errors (SSE):  [18.30289149]\n",
      "Training epoch#:  44\n",
      "errors (SSE):  [18.03735403]\n",
      "Training epoch#:  45\n",
      "errors (SSE):  [17.7833369]\n",
      "Training epoch#:  46\n",
      "errors (SSE):  [17.54028789]\n",
      "Training epoch#:  47\n",
      "errors (SSE):  [17.30769693]\n",
      "Training epoch#:  48\n",
      "errors (SSE):  [17.08509112]\n",
      "Training epoch#:  49\n",
      "errors (SSE):  [16.87203039]\n",
      "Training epoch#:  50\n",
      "errors (SSE):  [16.66810389]\n",
      "Training epoch#:  51\n",
      "errors (SSE):  [16.47292683]\n",
      "Training epoch#:  52\n",
      "errors (SSE):  [16.28613771]\n",
      "Training epoch#:  53\n",
      "errors (SSE):  [16.10739597]\n",
      "Training epoch#:  54\n",
      "errors (SSE):  [15.93637995]\n",
      "Training epoch#:  55\n",
      "errors (SSE):  [15.77278505]\n",
      "Training epoch#:  56\n",
      "errors (SSE):  [15.61632224]\n",
      "Training epoch#:  57\n",
      "errors (SSE):  [15.46671661]\n",
      "Training epoch#:  58\n",
      "errors (SSE):  [15.32370628]\n",
      "Training epoch#:  59\n",
      "errors (SSE):  [15.18704131]\n",
      "Training epoch#:  60\n",
      "errors (SSE):  [15.05648281]\n",
      "Training epoch#:  61\n",
      "errors (SSE):  [14.93180217]\n",
      "Training epoch#:  62\n",
      "errors (SSE):  [14.81278038]\n",
      "Training epoch#:  63\n",
      "errors (SSE):  [14.69920736]\n",
      "Training epoch#:  64\n",
      "errors (SSE):  [14.5908815]\n",
      "Training epoch#:  65\n",
      "errors (SSE):  [14.48760917]\n",
      "Training epoch#:  66\n",
      "errors (SSE):  [14.38920425]\n",
      "Training epoch#:  67\n",
      "errors (SSE):  [14.29548781]\n",
      "Training epoch#:  68\n",
      "errors (SSE):  [14.20628772]\n",
      "Training epoch#:  69\n",
      "errors (SSE):  [14.12143838]\n",
      "Training epoch#:  70\n",
      "errors (SSE):  [14.04078039]\n",
      "Training epoch#:  71\n",
      "errors (SSE):  [13.96416033]\n",
      "Training epoch#:  72\n",
      "errors (SSE):  [13.89143047]\n",
      "Training epoch#:  73\n",
      "errors (SSE):  [13.82244857]\n",
      "Training epoch#:  74\n",
      "errors (SSE):  [13.75707766]\n",
      "Training epoch#:  75\n",
      "errors (SSE):  [13.69518581]\n",
      "Training epoch#:  76\n",
      "errors (SSE):  [13.63664597]\n",
      "Training epoch#:  77\n",
      "errors (SSE):  [13.58133576]\n",
      "Training epoch#:  78\n",
      "errors (SSE):  [13.52913728]\n",
      "Training epoch#:  79\n",
      "errors (SSE):  [13.47993696]\n",
      "Training epoch#:  80\n",
      "errors (SSE):  [13.43362536]\n",
      "Training epoch#:  81\n",
      "errors (SSE):  [13.39009704]\n",
      "Training epoch#:  82\n",
      "errors (SSE):  [13.34925038]\n",
      "Training epoch#:  83\n",
      "errors (SSE):  [13.31098741]\n",
      "Training epoch#:  84\n",
      "errors (SSE):  [13.27521368]\n",
      "Training epoch#:  85\n",
      "errors (SSE):  [13.24183811]\n",
      "Training epoch#:  86\n",
      "errors (SSE):  [13.21077283]\n",
      "Training epoch#:  87\n",
      "errors (SSE):  [13.18193305]\n",
      "Training epoch#:  88\n",
      "errors (SSE):  [13.15523693]\n",
      "Training epoch#:  89\n",
      "errors (SSE):  [13.13060543]\n",
      "Training epoch#:  90\n",
      "errors (SSE):  [13.10796219]\n",
      "Training epoch#:  91\n",
      "errors (SSE):  [13.08723339]\n",
      "Training epoch#:  92\n",
      "errors (SSE):  [13.06834769]\n",
      "Training epoch#:  93\n",
      "errors (SSE):  [13.05123601]\n",
      "Training epoch#:  94\n",
      "errors (SSE):  [13.03583152]\n",
      "Training epoch#:  95\n",
      "errors (SSE):  [13.02206947]\n",
      "Training epoch#:  96\n",
      "errors (SSE):  [13.00988711]\n",
      "Training epoch#:  97\n",
      "errors (SSE):  [12.99922359]\n",
      "Training epoch#:  98\n",
      "errors (SSE):  [12.99001989]\n",
      "Training epoch#:  99\n",
      "errors (SSE):  [12.98221867]\n",
      "Training epoch#:  100\n",
      "errors (SSE):  [12.97576426]\n",
      "Training epoch#:  101\n",
      "errors (SSE):  [12.97060253]\n",
      "Training epoch#:  102\n",
      "errors (SSE):  [12.96668085]\n",
      "Training epoch#:  103\n",
      "errors (SSE):  [12.96394801]\n",
      "Training epoch#:  104\n",
      "errors (SSE):  [12.96235413]\n",
      "Training epoch#:  105\n",
      "errors (SSE):  [12.96185066]\n",
      "Training epoch#:  106\n",
      "errors (SSE):  [12.96239027]\n",
      "Training epoch#:  107\n",
      "errors (SSE):  [12.96392683]\n",
      "Training epoch#:  108\n",
      "errors (SSE):  [12.96641535]\n",
      "Training epoch#:  109\n",
      "errors (SSE):  [12.96981195]\n",
      "Training epoch#:  110\n",
      "errors (SSE):  [12.97407382]\n",
      "Training epoch#:  111\n",
      "errors (SSE):  [12.97915919]\n",
      "Training epoch#:  112\n",
      "errors (SSE):  [12.98502729]\n",
      "Training epoch#:  113\n",
      "errors (SSE):  [12.99163832]\n",
      "Training epoch#:  114\n",
      "errors (SSE):  [12.99895345]\n",
      "Training epoch#:  115\n",
      "errors (SSE):  [13.0069348]\n",
      "Training epoch#:  116\n",
      "errors (SSE):  [13.01554537]\n",
      "Training epoch#:  117\n",
      "errors (SSE):  [13.02474911]\n",
      "Training epoch#:  118\n",
      "errors (SSE):  [13.03451085]\n",
      "Training epoch#:  119\n",
      "errors (SSE):  [13.04479629]\n",
      "Training epoch#:  120\n",
      "errors (SSE):  [13.05557204]\n",
      "Training epoch#:  121\n",
      "errors (SSE):  [13.06680555]\n",
      "Training epoch#:  122\n",
      "errors (SSE):  [13.07846515]\n",
      "Training epoch#:  123\n",
      "errors (SSE):  [13.09052004]\n",
      "Training epoch#:  124\n",
      "errors (SSE):  [13.10294028]\n",
      "Training epoch#:  125\n",
      "errors (SSE):  [13.11569677]\n",
      "Training epoch#:  126\n",
      "errors (SSE):  [13.12876128]\n",
      "Training epoch#:  127\n",
      "errors (SSE):  [13.14210646]\n",
      "Training epoch#:  128\n",
      "errors (SSE):  [13.15570577]\n",
      "Training epoch#:  129\n",
      "errors (SSE):  [13.16953355]\n",
      "Training epoch#:  130\n",
      "errors (SSE):  [13.18356499]\n",
      "Training epoch#:  131\n",
      "errors (SSE):  [13.19777612]\n",
      "Training epoch#:  132\n",
      "errors (SSE):  [13.21214384]\n",
      "Training epoch#:  133\n",
      "errors (SSE):  [13.22664586]\n",
      "Training epoch#:  134\n",
      "errors (SSE):  [13.24126075]\n",
      "Training epoch#:  135\n",
      "errors (SSE):  [13.25596791]\n",
      "Training epoch#:  136\n",
      "errors (SSE):  [13.27074758]\n",
      "Training epoch#:  137\n",
      "errors (SSE):  [13.2855808]\n",
      "Training epoch#:  138\n",
      "errors (SSE):  [13.30044943]\n",
      "Training epoch#:  139\n",
      "errors (SSE):  [13.31533615]\n",
      "Training epoch#:  140\n",
      "errors (SSE):  [13.33022441]\n",
      "Training epoch#:  141\n",
      "errors (SSE):  [13.34509846]\n",
      "Training epoch#:  142\n",
      "errors (SSE):  [13.35994331]\n",
      "Training epoch#:  143\n",
      "errors (SSE):  [13.37474474]\n",
      "Training epoch#:  144\n",
      "errors (SSE):  [13.38948928]\n",
      "Training epoch#:  145\n",
      "errors (SSE):  [13.40416415]\n",
      "Training epoch#:  146\n",
      "errors (SSE):  [13.41875733]\n",
      "Training epoch#:  147\n",
      "errors (SSE):  [13.43325746]\n",
      "Training epoch#:  148\n",
      "errors (SSE):  [13.4476539]\n",
      "Training epoch#:  149\n",
      "errors (SSE):  [13.46193663]\n",
      "Training epoch#:  150\n",
      "errors (SSE):  [13.4760963]\n",
      "Training epoch#:  151\n",
      "errors (SSE):  [13.49012417]\n",
      "Training epoch#:  152\n",
      "errors (SSE):  [13.50401213]\n",
      "Training epoch#:  153\n",
      "errors (SSE):  [13.51775262]\n",
      "Training epoch#:  154\n",
      "errors (SSE):  [13.53133868]\n",
      "Training epoch#:  155\n",
      "errors (SSE):  [13.54476389]\n",
      "Training epoch#:  156\n",
      "errors (SSE):  [13.55802234]\n",
      "Training epoch#:  157\n",
      "errors (SSE):  [13.57110865]\n",
      "Training epoch#:  158\n",
      "errors (SSE):  [13.58401791]\n",
      "Training epoch#:  159\n",
      "errors (SSE):  [13.59674569]\n",
      "Training epoch#:  160\n",
      "errors (SSE):  [13.609288]\n",
      "Training epoch#:  161\n",
      "errors (SSE):  [13.62164128]\n",
      "Training epoch#:  162\n",
      "errors (SSE):  [13.63380239]\n",
      "Training epoch#:  163\n",
      "errors (SSE):  [13.64576857]\n",
      "Training epoch#:  164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors (SSE):  [13.65753743]\n",
      "Training epoch#:  165\n",
      "errors (SSE):  [13.66910694]\n",
      "Training epoch#:  166\n",
      "errors (SSE):  [13.68047542]\n",
      "Training epoch#:  167\n",
      "errors (SSE):  [13.69164147]\n",
      "Training epoch#:  168\n",
      "errors (SSE):  [13.70260404]\n",
      "Training epoch#:  169\n",
      "errors (SSE):  [13.71336233]\n",
      "Training epoch#:  170\n",
      "errors (SSE):  [13.72391583]\n",
      "Training epoch#:  171\n",
      "errors (SSE):  [13.73426426]\n",
      "Training epoch#:  172\n",
      "errors (SSE):  [13.74440762]\n",
      "Training epoch#:  173\n",
      "errors (SSE):  [13.75434609]\n",
      "Training epoch#:  174\n",
      "errors (SSE):  [13.76408009]\n",
      "Training epoch#:  175\n",
      "errors (SSE):  [13.77361024]\n",
      "Training epoch#:  176\n",
      "errors (SSE):  [13.78293731]\n",
      "Training epoch#:  177\n",
      "errors (SSE):  [13.79206229]\n",
      "Training epoch#:  178\n",
      "errors (SSE):  [13.80098628]\n",
      "Training epoch#:  179\n",
      "errors (SSE):  [13.80971058]\n",
      "Training epoch#:  180\n",
      "errors (SSE):  [13.8182366]\n",
      "Training epoch#:  181\n",
      "errors (SSE):  [13.82656586]\n",
      "Training epoch#:  182\n",
      "errors (SSE):  [13.83470004]\n",
      "Training epoch#:  183\n",
      "errors (SSE):  [13.8426409]\n",
      "Training epoch#:  184\n",
      "errors (SSE):  [13.8503903]\n",
      "Training epoch#:  185\n",
      "errors (SSE):  [13.8579502]\n",
      "Training epoch#:  186\n",
      "errors (SSE):  [13.86532263]\n",
      "Training epoch#:  187\n",
      "errors (SSE):  [13.87250972]\n",
      "Training epoch#:  188\n",
      "errors (SSE):  [13.87951363]\n",
      "Training epoch#:  189\n",
      "errors (SSE):  [13.88633662]\n",
      "Training epoch#:  190\n",
      "errors (SSE):  [13.89298096]\n",
      "Training epoch#:  191\n",
      "errors (SSE):  [13.89944902]\n",
      "Training epoch#:  192\n",
      "errors (SSE):  [13.90574315]\n",
      "Training epoch#:  193\n",
      "errors (SSE):  [13.9118658]\n",
      "Training epoch#:  194\n",
      "errors (SSE):  [13.9178194]\n",
      "Training epoch#:  195\n",
      "errors (SSE):  [13.92360643]\n",
      "Training epoch#:  196\n",
      "errors (SSE):  [13.92922939]\n",
      "Training epoch#:  197\n",
      "errors (SSE):  [13.93469079]\n",
      "Training epoch#:  198\n",
      "errors (SSE):  [13.93999316]\n",
      "Training epoch#:  199\n",
      "errors (SSE):  [13.94513903]\n",
      "Training epoch#:  200\n",
      "errors (SSE):  [13.95013095]\n",
      "Training epoch#:  201\n",
      "errors (SSE):  [13.95497146]\n",
      "Training epoch#:  202\n",
      "errors (SSE):  [13.95966311]\n",
      "Training epoch#:  203\n",
      "errors (SSE):  [13.96420843]\n",
      "Training epoch#:  204\n",
      "errors (SSE):  [13.96860996]\n",
      "Training epoch#:  205\n",
      "errors (SSE):  [13.97287023]\n",
      "Training epoch#:  206\n",
      "errors (SSE):  [13.97699174]\n",
      "Training epoch#:  207\n",
      "errors (SSE):  [13.98097701]\n",
      "Training epoch#:  208\n",
      "errors (SSE):  [13.98482851]\n",
      "Training epoch#:  209\n",
      "errors (SSE):  [13.98854873]\n",
      "Training epoch#:  210\n",
      "errors (SSE):  [13.9921401]\n",
      "Training epoch#:  211\n",
      "errors (SSE):  [13.99560507]\n",
      "Training epoch#:  212\n",
      "errors (SSE):  [13.99894604]\n",
      "Training epoch#:  213\n",
      "errors (SSE):  [14.00216541]\n",
      "Training epoch#:  214\n",
      "errors (SSE):  [14.00526553]\n",
      "Training epoch#:  215\n",
      "errors (SSE):  [14.00824876]\n",
      "Training epoch#:  216\n",
      "errors (SSE):  [14.01111741]\n",
      "Training epoch#:  217\n",
      "errors (SSE):  [14.01387377]\n",
      "Training epoch#:  218\n",
      "errors (SSE):  [14.0165201]\n",
      "Training epoch#:  219\n",
      "errors (SSE):  [14.01905864]\n",
      "Training epoch#:  220\n",
      "errors (SSE):  [14.02149161]\n",
      "Training epoch#:  221\n",
      "errors (SSE):  [14.02382117]\n",
      "Training epoch#:  222\n",
      "errors (SSE):  [14.02604949]\n",
      "Training epoch#:  223\n",
      "errors (SSE):  [14.02817868]\n",
      "Training epoch#:  224\n",
      "errors (SSE):  [14.03021085]\n",
      "Training epoch#:  225\n",
      "errors (SSE):  [14.03214805]\n",
      "Training epoch#:  226\n",
      "errors (SSE):  [14.03399234]\n",
      "Training epoch#:  227\n",
      "errors (SSE):  [14.0357457]\n",
      "Training epoch#:  228\n",
      "errors (SSE):  [14.03741012]\n",
      "Training epoch#:  229\n",
      "errors (SSE):  [14.03898755]\n",
      "Training epoch#:  230\n",
      "errors (SSE):  [14.0404799]\n",
      "Training epoch#:  231\n",
      "errors (SSE):  [14.04188907]\n",
      "Training epoch#:  232\n",
      "errors (SSE):  [14.04321691]\n",
      "Training epoch#:  233\n",
      "errors (SSE):  [14.04446526]\n",
      "Training epoch#:  234\n",
      "errors (SSE):  [14.04563592]\n",
      "Training epoch#:  235\n",
      "errors (SSE):  [14.04673066]\n",
      "Training epoch#:  236\n",
      "errors (SSE):  [14.04775122]\n",
      "Training epoch#:  237\n",
      "errors (SSE):  [14.04869933]\n",
      "Training epoch#:  238\n",
      "errors (SSE):  [14.04957668]\n",
      "Training epoch#:  239\n",
      "errors (SSE):  [14.05038491]\n",
      "Training epoch#:  240\n",
      "errors (SSE):  [14.05112568]\n",
      "Training epoch#:  241\n",
      "errors (SSE):  [14.05180058]\n",
      "Training epoch#:  242\n",
      "errors (SSE):  [14.05241119]\n",
      "Training epoch#:  243\n",
      "errors (SSE):  [14.05295907]\n",
      "Training epoch#:  244\n",
      "errors (SSE):  [14.05344574]\n",
      "Training epoch#:  245\n",
      "errors (SSE):  [14.05387271]\n",
      "Training epoch#:  246\n",
      "errors (SSE):  [14.05424144]\n",
      "Training epoch#:  247\n",
      "errors (SSE):  [14.0545534]\n",
      "Training epoch#:  248\n",
      "errors (SSE):  [14.05481]\n",
      "Training epoch#:  249\n",
      "errors (SSE):  [14.05501265]\n",
      "Training epoch#:  250\n",
      "errors (SSE):  [14.05516271]\n",
      "Training epoch#:  251\n",
      "errors (SSE):  [14.05526155]\n",
      "Training epoch#:  252\n",
      "errors (SSE):  [14.05531049]\n",
      "Training epoch#:  253\n",
      "errors (SSE):  [14.05531084]\n",
      "Training epoch#:  254\n",
      "errors (SSE):  [14.05526387]\n",
      "Training epoch#:  255\n",
      "errors (SSE):  [14.05517084]\n",
      "Training epoch#:  256\n",
      "errors (SSE):  [14.055033]\n",
      "Training epoch#:  257\n",
      "errors (SSE):  [14.05485155]\n",
      "Training epoch#:  258\n",
      "errors (SSE):  [14.05462769]\n",
      "Training epoch#:  259\n",
      "errors (SSE):  [14.05436259]\n",
      "[[0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [1, 1], [0, 0], [0, 1], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [1, 0], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 1], [1, 1], [1, 1], [0, 0], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "hybrid = ANNkNNHybrid(datasets['breast_cancer']['inputnodes'],\n",
    "                   datasets['breast_cancer']['hiddennodes'],\n",
    "                   datasets['breast_cancer']['outputnodes'],\n",
    "                   datasets['breast_cancer']['lr'],\n",
    "                   datasets['breast_cancer']['bs'],\n",
    "                   datasets['breast_cancer']['epochs'],\n",
    "                      datasets['breast_cancer']['k'],\n",
    "         weighted=datasets['breast_cancer']['weighted'])\n",
    "hybrid.train(datasets['breast_cancer']['train'])\n",
    "#Test\n",
    "hybrid.test(datasets['breast_cancer']['test'])\n",
    "breast_cancer_results.append(accuracy(hybrid.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparrison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wine Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Model Accuracy Comparison')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [\"ANN\", \"kNN\", \"ANN-kNN Hybrid\"]\n",
    "y_pos = np.arange(len(labels))\n",
    "plt.bar(y_pos,wine_results,align=\"center\",alpha=0.5)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51.65373621886484, 58.391180073499385, 53.2053899550837]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, kNN was most accurate followed by ANN and finally the Hybrid. However the difference in accuracy between the ANN and the Hybrid is very small "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Breast Cancer Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Model Accuracy Comparison')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [\"ANN\", \"kNN\", \"ANN-kNN Hybrid\"]\n",
    "y_pos = np.arange(len(labels))\n",
    "plt.bar(y_pos,breast_cancer_results,align=\"center\",alpha=0.5)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97.1830985915493, 94.38596491228071, 98.23943661971832]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the Hybrid was the most accurate followed by ANN and finally kNN. Again similar to Wine dataset the difference betweeen the ANN and the hybrid accuracies are very small "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
